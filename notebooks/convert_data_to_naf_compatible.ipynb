{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from typing import List\n",
    "from scipy.fftpack import rfft, irfft\n",
    "import spaudiopy as spa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "from spatial_sampling.dataloader import parse_room_data, SpatialRoomDataset, load_dataset\n",
    "from spatial_sampling.config import SpatialSamplingConfig\n",
    "from src.sofa_parser import HRIRSOFAReader, SRIRSOFAWriter\n",
    "from src.sound_examples import binaural_dynamic_rendering\n",
    "from src.run_model import load_and_validate_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4646d-19df-48ee-ad1a-476ee784167a",
   "metadata": {},
   "source": [
    "### Notebook to convert coupled room dataset to NAF compatible\n",
    "\n",
    "NAF takes in BRIRs at spatial locations in the room for head orientations [0, 90, 180, 270]. To train NAF with different subsets of receivers, like we do in the WASPAA paper, we create different training and inference dataset containing SRIRs at different locations, and then convert them to BRIRs for the 4 given orientations. Finally we save them in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'output/spatial_sampling/sound_examples'\n",
    "room_data_pkl_path = Path('resources/Georg_3room_FDTD/srirs_spatial.pkl').resolve()\n",
    "config_path = Path('data/config/spatial_sampling/').resolve()\n",
    "save_path = Path('resources/Georg_3room_FDTD').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NAFDataset:\n",
    "    num_train_receivers: int\n",
    "    num_infer_receivers: int\n",
    "    train_receiver_pos: NDArray #of shape num_training_receivers x 3\n",
    "    infer_receiver_pos: NDArray #of shape num_infer_receivers x 3\n",
    "    train_brirs: NDArray #of shape num_training_receivers x num_orientation x num_time_samples x num_ears\n",
    "    infer_brirs: NDArray #of shape num_infer_receivers x num_orientation x num_time_samples  x num_ears \n",
    "    orientation: ArrayLike # of length 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rec_idx_in_room_dataset(room_data : SpatialRoomDataset, rec_pos_list: NDArray) -> List:\n",
    "    \"\"\"Indices of the receivers in the dataset associated with the list of receiver positions\"\"\"\n",
    "    # Compute Euclidean distance between each array in array_list_np and every row in matrix\n",
    "\n",
    "    distances = np.linalg.norm(room_data.receiver_position[:, None, :] -\n",
    "                               rec_pos_list,\n",
    "                               axis=2)\n",
    "    indices = np.argmin(distances, axis=0)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_srir_to_brir(srirs:NDArray, sample_rate: float, hrtf_reader:HRIRSOFAReader, head_orientations: ArrayLike) -> NDArray:\n",
    "    \"\"\"\n",
    "    Convert SRIRs to BRIRs for specific orientations\n",
    "    Args:\n",
    "        srirs (NDArray): SRIRs of shape num_pos x num_ambi_channels x num_time_samp\n",
    "        sample_rate (float): sample rate of the SRIRs\n",
    "        hrtf_reader (HRIRSOFAReader): for parsing SOFA file\n",
    "        head_orientations (ArrayLike): head orientations of shape num_ori x  2\n",
    "    Returns:\n",
    "        BRIRs of shape num_pos x num_ori x num_time_samples x 2\n",
    "    \"\"\"\n",
    "    ambi_order = int(np.sqrt(srirs.shape[1] - 1))\n",
    "    num_receivers = srirs.shape[0]\n",
    "    num_freq_bins = 2**int(np.ceil(np.log2(srirs.shape[-1])))\n",
    "    \n",
    "    # size is num_ambi_channels x num_receivers x num_time_samples\n",
    "    hrir_sh = hrtf_reader.get_spherical_harmonic_representation(ambi_order)\n",
    "    ambi_rtfs = rfft(srirs, num_freq_bins, axis=-1)\n",
    "\n",
    "    # these are of shape num_ambi_channels x 2 x num_freq_samples\n",
    "    ambi_hrtfs = rfft(hrir_sh, n=num_freq_bins, axis=-1)\n",
    "    logger.info(\"Done calculating FFTs\")\n",
    "\n",
    "    num_orientations = head_orientations.shape[0]\n",
    "    brirs = np.zeros((num_receivers, num_orientations, num_freq_bins, 2))\n",
    "\n",
    "    for rec_pos_idx in tqdm(range(num_receivers)):\n",
    "        # shape is num_ambi_channels x num_freqs\n",
    "        cur_ambi_rtf = ambi_rtfs[rec_pos_idx, ...]\n",
    "\n",
    "        for ori_idx in range(num_orientations):\n",
    "            cur_head_orientation = head_orientations[ori_idx, :]\n",
    "    \n",
    "            #rotate the soundfield in the opposite direction - size num_freq_bins x num_ambi_channels\n",
    "            cur_rotation_matrix = spa.sph.sh_rotation_matrix(\n",
    "                ambi_order,\n",
    "                -cur_head_orientation[0],\n",
    "                -cur_head_orientation[1],\n",
    "                0,\n",
    "                sh_type='real')\n",
    "    \n",
    "            rotated_ambi_rtf = cur_ambi_rtf.T @ cur_rotation_matrix.T\n",
    "    \n",
    "            # get the binaural room transfer function\n",
    "            cur_brtf = np.einsum('nrf, fn -> fr', np.conj(ambi_hrtfs),\n",
    "                                 rotated_ambi_rtf)\n",
    "            # get the BRIR\n",
    "            cur_brir = irfft(cur_brtf, n=num_freq_bins, axis=0)\n",
    "            brirs[rec_pos_idx, ori_idx, ...] = cur_brir\n",
    "    \n",
    "    return brirs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Get the true room dataset for different grid spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = f'{config_path}/treble_data_grid_training_1000Hz_directional_spatial_sampling_test.yml'\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       SpatialSamplingConfig)\n",
    "hrtf_path = Path('resources/HRTF/48kHz/KEMAR_Knowl_EarSim_SmallEars_FreeFieldComp_48kHz.sofa')\n",
    "\n",
    "# get the original dataset\n",
    "room_data = parse_room_data(room_data_pkl_path)\n",
    "\n",
    "# get the HRTF\n",
    "hrtf_reader = HRIRSOFAReader(hrtf_path)\n",
    "\n",
    "if hrtf_reader.fs != room_data.sample_rate:\n",
    "    logger.info(\n",
    "            f\"Resampling HRTFs to {room_data.sample_rate:.0f} Hz\")\n",
    "    hrtf_reader.resample_hrirs(room_data.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get train dataset for different grid spacings\n",
    "grid_resolution_m = np.arange(config_dict.num_grid_spacing, 0,\n",
    "                                  -1) * room_data.grid_spacing_m\n",
    "head_orientations = np.zeros((4, 2))\n",
    "# these are the only directions in NAF\n",
    "head_orientations[:, 0] = np.array([0, 90, 180, 270])\n",
    "\n",
    "for k in range(config_dict.num_grid_spacing):\n",
    "    logger.info(f'Creatng NAF dataset for grid spacing = {np.round(grid_resolution_m[k], 1)}m')\n",
    "\n",
    "    pkl_path = f'{save_path}/naf_dataset_grid_spacing={grid_resolution_m[k]:.1f}m.pkl'\n",
    "\n",
    "    if not os.path.exists(pkl_path):\n",
    "        all_train_rec_pos = []\n",
    "        all_train_srir = []\n",
    "        all_valid_rec_pos = []\n",
    "        all_valid_srir = []\n",
    "        \n",
    "        # prepare the training and validation data for DiffGFDN\n",
    "        train_dataset, valid_dataset, dataset_ref = load_dataset(\n",
    "            room_data,\n",
    "            config_dict.device,\n",
    "            grid_resolution_m=np.round(grid_resolution_m[k], 1),\n",
    "            network_type=config_dict.network_type,\n",
    "            batch_size=config_dict.batch_size)\n",
    "    \n",
    "        logger.info(\"Creating training BRIRs\")\n",
    "        # training data\n",
    "        for data in train_dataset:\n",
    "            cur_list_pos = data['listener_position'].detach().cpu().numpy()\n",
    "            all_train_rec_pos.append(cur_list_pos)\n",
    "            indx = find_rec_idx_in_room_dataset(room_data, cur_list_pos)\n",
    "            cur_srir = room_data.rirs[indx, ...]\n",
    "            all_train_srir.append(cur_srir)\n",
    "    \n",
    "        train_srir = np.vstack(all_train_srir)\n",
    "        train_pos = np.vstack(all_train_rec_pos)\n",
    "        train_brirs = convert_srir_to_brir(train_srir, room_data.sample_rate, hrtf_reader, head_orientations)\n",
    "    \n",
    "        logger.info(\"Creating inference BRIRs\")\n",
    "        # inference data\n",
    "        if grid_resolution_m[k] != room_data.grid_spacing_m\n",
    "            for data in valid_dataset:\n",
    "                cur_list_pos = data['listener_position'].detach().cpu().numpy()\n",
    "                all_valid_rec_pos.append(cur_list_pos)\n",
    "                indx = find_rec_idx_in_room_dataset(room_data, cur_list_pos)\n",
    "                cur_srir = room_data.rirs[indx, ...]\n",
    "                all_valid_srir.append(cur_srir)\n",
    "        \n",
    "            valid_srir = np.vstack(all_valid_srir)\n",
    "            valid_pos = np.vstack(all_valid_rec_pos)\n",
    "            valid_brirs = convert_srir_to_brir(valid_srir, room_data.sample_rate, hrtf_reader, head_orientations)\n",
    "            num_valid_receivers = valid_pos.shape[0]\n",
    "        else:\n",
    "            valid_srir = None\n",
    "            valid_pos = None\n",
    "            valid_brirs = None\n",
    "            num_valid_receivers = None\n",
    "        \n",
    "    \n",
    "        logger.info(\"Creating NAF dataset\")\n",
    "        naf_dataset = NAFDataset(num_train_receivers = train_pos.shape[0],\n",
    "                                 num_infer_receivers = num_valid_receivers,\n",
    "                                 train_receiver_pos = train_pos,\n",
    "                                 infer_receiver_pos = valid_pos,\n",
    "                                 train_brirs = train_brirs,\n",
    "                                 infer_brirs = valid_brirs,\n",
    "                                 orientation = head_orientations[0, :],                      \n",
    "                                )\n",
    "        print(naf_dataset.train_brirs.shape, naf_dataset.train_receiver_pos.shape)\n",
    "    \n",
    "        # Step 3: Save the instance to a pickle file\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(naf_dataset, f)\n",
    "    else:\n",
    "        logger.info(\"File already exists!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
