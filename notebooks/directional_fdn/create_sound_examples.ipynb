{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyfar as pf\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "from pathlib import Path\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from typing import Optional, List\n",
    "import IPython\n",
    "from loguru import logger\n",
    "from copy import deepcopy\n",
    "\n",
    "os.chdir('../..')  # This changes the working directory to DiffGFDN\n",
    "\n",
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.config.config_loader import load_and_validate_config\n",
    "from diff_gfdn.inference import infer_all_octave_bands_directional_fdn\n",
    "from diff_gfdn.utils import ms_to_samps\n",
    "from spatial_sampling.dataloader import SpatialThreeRoomDataset\n",
    "\n",
    "from src.sofa_parser import HRIRSOFAReader\n",
    "from src.sound_examples import binaural_dynamic_rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path('output/directional_fdn/').resolve()\n",
    "audio_path = Path('output/sound_examples').resolve()\n",
    "config_path = Path('data/config/directional_fdn').resolve()\n",
    "room_data_pkl_path = Path('resources/Georg_3room_FDTD/srirs_spatial.pkl').resolve()\n",
    "\n",
    "# get the original dataset\n",
    "room_data = SpatialThreeRoomDataset(room_data_pkl_path)\n",
    "\n",
    "freqs_list = [63, 125, 250, 500, 1000, 2000, 4000, 8000]\n",
    "grid_res_m = 0.6\n",
    "save_dir = f'/treble_data_directional_fdn_grid_res={grid_res_m:.1f}m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Create a trajectory of a listener moving across the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# along x axis between three rooms\n",
    "start_pos_x, start_pos_y = (0.5, 3.5)\n",
    "end_pos_x, end_pos_y = (9, 3.5)\n",
    "num_pos = 50\n",
    "head_orientation_az = np.deg2rad(np.linspace(200, 30, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "\n",
    "rec_pos_list = np.zeros((num_pos, 3))\n",
    "rec_pos_list[:, 0] = linear_trajectory_x\n",
    "rec_pos_list[:, 1] = linear_trajectory_y\n",
    "rec_pos_list[:, 2] = linear_trajectory_z\n",
    "orientation_list = np.zeros((num_pos, 2))\n",
    "orientation_list[:, 0] = head_orientation_az\n",
    "orientation_list[:, 1] = head_orientation_el\n",
    "\n",
    "# along y-axis between rooms 2 and 3\n",
    "start_pos_x, start_pos_y = (9.1, 3.5)\n",
    "end_pos_x, end_pos_y = (9.0, 12.0)\n",
    "num_pos = 68\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "head_orientation_az = np.deg2rad(np.linspace(30, 150, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "rec_pos_list = np.vstack((rec_pos_list, np.vstack((linear_trajectory_x, linear_trajectory_y, linear_trajectory_z)).T))\n",
    "head_orientation_list = np.vstack((orientation_list, np.vstack((head_orientation_az, head_orientation_el)).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Create config dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dicts = []\n",
    "\n",
    "for k in range(len(freqs_list)):\n",
    "    config_name = f'/treble_data_grid_training_{freqs_list[k]}Hz_directional_fdn_grid_res={grid_res_m:.1f}m.yml'\n",
    "    cur_config_dict = load_and_validate_config(str(config_path) + config_name, DiffGFDNConfig)\n",
    "    config_dicts.append(cur_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Get original and synthesised late reverberation by DiffDirectionalFDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the synthesised late tail\n",
    "synth_dfdn_room_data = infer_all_octave_bands_directional_fdn(freqs_list, \n",
    "                                                              config_dicts, \n",
    "                                                              str(out_path) + save_dir, \n",
    "                                                              room_data,\n",
    "                                                              rec_pos_list,\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rir = synth_dfdn_room_data.rirs[54, ...]\n",
    "plt.plot(rir.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Get the mono, dry stimulus and resample it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_type = 'speech'\n",
    "\n",
    "speech_data = pf.signals.files.drums() if sig_type == 'drums' else pf.signals.files.speech()\n",
    "speech = np.squeeze(speech_data.time)\n",
    "fs = speech_data.sampling_rate\n",
    "new_fs = int(synth_dfdn_room_data.sample_rate)\n",
    "\n",
    "if fs != new_fs:\n",
    "    speech = librosa.resample(speech, orig_sr = fs, target_sr = new_fs)\n",
    "\n",
    "# add some silence at the end\n",
    "silence = np.zeros(ms_to_samps(500, new_fs))\n",
    "speech_app = np.concatenate((speech, silence))\n",
    "IPython.display.Audio(speech_app, rate=new_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Load the HRTF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrtf_path = Path('resources/HRTF/48kHz/KEMAR_Knowl_EarSim_SmallEars_FreeFieldComp_48kHz.sofa')\n",
    "hrtf_reader = HRIRSOFAReader(hrtf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Create binaural example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ms = 250 #should be a factor of 1s\n",
    "ani_save_path = Path(f'{out_path}/sound_examples/treble_data_binaural').resolve()\n",
    "\n",
    "dynamic_renderer = binaural_dynamic_rendering(synth_dfdn_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "dynamic_renderer.animate_moving_listener(ani_save_path)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "pred_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "pred_output_norm = dynamic_renderer.normalise_loudness(pred_output, synth_dfdn_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/sound_examples/binaural_directional_fdn_no_gain_comp_grid_res={grid_res_m:.1f}_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, pred_output_norm, int(synth_dfdn_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', \n",
    "                                             f'{ani_save_path}_directional_fdn_no_gain_comp_grid_res={grid_res_m:.1f}_{sig_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
