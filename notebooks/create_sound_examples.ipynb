{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyfar as pf\n",
    "import soundfile as sf\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import pickle\n",
    "import IPython\n",
    "from numpy.typing import ArrayLike\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "from scipy.signal import fftconvolve\n",
    "from copy import deepcopy\n",
    "from loguru import logger\n",
    "\n",
    "os.chdir('..')  # This changes the working directory to DiffGFDN\n",
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset\n",
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.model import DiffGFDNVarReceiverPos\n",
    "from diff_gfdn.utils import ms_to_samps, db, get_response\n",
    "from diff_gfdn.plot import plot_spectrogram\n",
    "from diff_gfdn.losses import get_stft_torch\n",
    "from diff_gfdn.colorless_fdn.utils import get_colorless_fdn_params\n",
    "from slope2noise.generate import shaped_wgn\n",
    "\n",
    "\n",
    "from src.run_model import load_and_validate_config\n",
    "from src.run_subband_training_treble import sum_arrays\n",
    "from src.sound_examples import dynamic_rendering_moving_receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/'\n",
    "fig_path = 'figures/'\n",
    "audio_path = 'audio/sound_examples/'\n",
    "out_path = 'output/'\n",
    "config_name = 'treble_data_grid_training_full_band_colorless_loss'\n",
    "config_file = config_path + f'{config_name}.yml'\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)\n",
    "room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve(), config_dict)\n",
    "room_data.mixing_time_ms = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Get the stimulus and resample it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.utils import ms_to_samps\n",
    "\n",
    "sig_type = 'drums'\n",
    "\n",
    "speech_data = pf.signals.files.drums() if sig_type == 'drums' else pf.signals.files.speech()\n",
    "speech = np.squeeze(speech_data.time)\n",
    "fs = speech_data.sampling_rate\n",
    "\n",
    "if fs != room_data.sample_rate:\n",
    "    speech = librosa.resample(speech, orig_sr = fs, target_sr = room_data.sample_rate)\n",
    "\n",
    "# add some silence at the end\n",
    "silence = np.zeros(ms_to_samps(500, room_data.sample_rate))\n",
    "speech_app = np.concatenate((speech, silence))\n",
    "                   \n",
    "save_path = Path(f'{audio_path}/stimulus/{sig_type}.wav').resolve()\n",
    "sf.write(save_path, speech_app, room_data.sample_rate) \n",
    "IPython.display.Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Create a trajectory of a listener moving across the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# along x axis between three rooms\n",
    "start_pos_x, start_pos_y = (0.5, 3.5)\n",
    "end_pos_x, end_pos_y = (9, 3.5)\n",
    "num_pos = 50\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "\n",
    "rec_pos_list = np.zeros((num_pos, 3))\n",
    "rec_pos_list[:, 0] = linear_trajectory_x\n",
    "rec_pos_list[:, 1] = linear_trajectory_y\n",
    "rec_pos_list[:, 2] = linear_trajectory_z\n",
    "\n",
    "# along y-axis between rooms 2 and 3\n",
    "start_pos_x, start_pos_y = (9.1, 3.5)\n",
    "end_pos_x, end_pos_y = (9.0, 12.0)\n",
    "num_pos = 68\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "\n",
    "rec_pos_list = np.vstack((rec_pos_list, np.vstack((linear_trajectory_x, linear_trajectory_y, linear_trajectory_z)).T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Get the common slope RIRs for all measured positions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_pickle_path = Path(f'{out_path}/treble_data_grid_common_slopes.pkl').resolve()\n",
    "\n",
    "if not os.path.exists(cs_pickle_path):\n",
    "    # synthesise for all positions - this is slow\n",
    "    decay_times = np.squeeze(room_data.common_decay_times)\n",
    "    ir_length_samps = int(2 * room_data.sample_rate)\n",
    "    t_vals_expanded = np.repeat(np.array(decay_times.T)[np.newaxis, ...],\n",
    "                                        room_data.num_rec,\n",
    "                                        axis=0)\n",
    "    batch_size = room_data.num_rec\n",
    "    num_batches = int(np.ceil(float(room_data.num_rec) / batch_size))\n",
    "    ls_est_rirs = np.zeros((room_data.num_rec, ir_length_samps))\n",
    "    \n",
    "    for n in range(num_batches):\n",
    "        batch_idx = np.arange(n * batch_size,\n",
    "                              max(room_data.num_rec, (n + 1) * batch_size),\n",
    "                              dtype=np.int32)\n",
    "        _, ls_est_rirs[batch_idx, :] = shaped_wgn(t_vals_expanded[batch_idx, ...], \n",
    "                                                  room_data.amplitudes[batch_idx, ...], \n",
    "                                                  room_data.sample_rate, \n",
    "                                                  ir_length_samps, \n",
    "                                                  room_data.band_centre_hz, \n",
    "                                                  # n_vals=np.squeeze(room_data.noise_floor[batch_idx, ...])\n",
    "                                                 )\n",
    "    # update the RIRs\n",
    "    cs_room_data = deepcopy(room_data)\n",
    "    cs_room_data.update_rirs(ls_est_rirs)\n",
    "    \n",
    "    # Save to a file\n",
    "    with open(cs_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(cs_room_data, f)\n",
    "else:\n",
    "    with open(cs_pickle_path, \"rb\") as f:\n",
    "        cs_room_data= pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Get the full band DiffGFDN solution at the trajectory positions - note that the early rirs are all wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gfdn_pickle_path = Path(f'{out_path}/sound_examples/treble_data_moving_listener_fullband_gfdn.pkl').resolve()\n",
    "\n",
    "if not os.path.exists(full_gfdn_pickle_path):\n",
    "    # add number of groups to the config dictionary\n",
    "    config_dict = config_dict.model_copy(update={\"num_groups\": room_data.num_rooms})\n",
    "    \n",
    "    if config_dict.sample_rate != room_data.sample_rate:\n",
    "        logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "        config_dict.sample_rate = sample_rate\n",
    "    \n",
    "    # get the training config\n",
    "    trainer_config = config_dict.trainer_config\n",
    "    \n",
    "    # force the trainer config device to be CPU\n",
    "    if trainer_config.device != 'cpu':\n",
    "        trainer_config = trainer_config.model_copy(update={\"device\": 'cpu'})\n",
    "    \n",
    "    full_gfdn_room_data = deepcopy(room_data)\n",
    "    full_gfdn_room_data.update_receiver_pos(rec_pos_list)\n",
    "    \n",
    "    # prepare the training and validation data for DiffGFDN\n",
    "    train_dataset, valid_dataset = load_dataset(\n",
    "        full_gfdn_room_data, trainer_config.device, train_valid_split_ratio=1.0,\n",
    "        batch_size=trainer_config.batch_size, shuffle=False)\n",
    "    \n",
    "    # initialise the model\n",
    "    model = DiffGFDNVarReceiverPos(full_gfdn_room_data.sample_rate, \n",
    "                                   full_gfdn_room_data.num_rooms,\n",
    "                                   config_dict.delay_length_samps,\n",
    "                                   trainer_config.device, \n",
    "                                   config_dict.feedback_loop_config,\n",
    "                                   config_dict.output_filter_config,\n",
    "                                   config_dict.decay_filter_config.use_absorption_filters,\n",
    "                                   common_decay_times=full_gfdn_room_data.common_decay_times,\n",
    "                                   band_centre_hz=full_gfdn_room_data.band_centre_hz,\n",
    "                                )\n",
    "    \n",
    "    # load the trained weights for the particular epoch\n",
    "    max_epochs = trainer_config.max_epochs\n",
    "    checkpoint_dir = Path(trainer_config.train_dir + 'checkpoints/').resolve()\n",
    "    checkpoint = torch.load(f'{checkpoint_dir}/model_e{max_epochs-1}.pt', weights_only=True, map_location=torch.device('cpu'))\n",
    "    # Load the trained model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    all_fullband_pos = []\n",
    "    all_fullband_rirs = []\n",
    "    \n",
    "    for data in train_dataset:\n",
    "        position = data['listener_position']\n",
    "        H, h = get_response(data, model)    \n",
    "        for num_pos in range(position.shape[0]):\n",
    "            # collate all RIRs at all positions\n",
    "            all_fullband_pos.append(position[num_pos])\n",
    "            all_fullband_rirs.append(h[num_pos, ...])\n",
    "\n",
    "    full_gfdn_room_data.update_receiver_pos(np.asarray(all_fullband_pos))\n",
    "    full_gfdn_room_data.update_rirs(np.asarray(all_fullband_rirs))\n",
    "\n",
    "    # Save to a file\n",
    "    with open(full_gfdn_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(full_gfdn_room_data, f)\n",
    "    \n",
    "else:\n",
    "    with open(full_gfdn_pickle_path, \"rb\") as f:\n",
    "        full_gfdn_room_data= pickle.load(f)\n",
    "\n",
    "\n",
    "# some plotting to investigate\n",
    "rec_pos_idx = 34\n",
    "dist = np.linalg.norm(full_gfdn_room_data.receiver_position[rec_pos_idx, :] - room_data.receiver_position, axis=-1)\n",
    "ref_rir_idx = np.argmin(dist, axis=0)\n",
    "plt.plot(full_gfdn_room_data.late_rirs[rec_pos_idx, :])\n",
    "plt.plot(room_data.late_rirs[ref_rir_idx, :])\n",
    "\n",
    "S_true, freqs, time_frames = get_stft_torch(torch.tensor(room_data.late_rirs[rec_pos_idx, :]), \n",
    "                                       room_data.sample_rate, win_size=2**12, hop_size=2**11, nfft=2**12)\n",
    "S, freqs, time_frames = get_stft_torch(torch.tensor(full_gfdn_room_data.late_rirs[rec_pos_idx, :]), \n",
    "                                       room_data.sample_rate, win_size=2**12, hop_size=2**11, nfft=2**12)\n",
    "\n",
    "plot_spectrogram(db(torch.abs(S_true)), freqs, time_frames, title='Ref RIR', log_freq_axis=True)\n",
    "plot_spectrogram(db(torch.abs(S)), freqs, time_frames, title='Fullband DiffGFDN', log_freq_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Get the subband DiffGFDN solution at the trajectory positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "subband_gfdn_pickle_path = Path(f'{out_path}/sound_examples/treble_data_moving_listener_subband_gfdn.pkl').resolve()\n",
    "\n",
    "if not os.path.exists(subband_gfdn_pickle_path):\n",
    "    subband_filters, _ = pf.dsp.filter.reconstructing_fractional_octave_bands(\n",
    "        None,\n",
    "        num_fractions=1,\n",
    "        frequency_range=(room_data.band_centre_hz[0], room_data.band_centre_hz[-1]),\n",
    "        sampling_rate=room_data.sample_rate,\n",
    "    )\n",
    "    \n",
    "    synth_subband_rirs = pd.DataFrame(columns=[\n",
    "        'frequency', 'position', 'filtered_time_samples'\n",
    "    ])\n",
    "\n",
    "    # loop through all subband frequencies\n",
    "    for k in range(len(room_data.band_centre_hz)):\n",
    "        logger.info(\n",
    "            f'Running inferencing for subband = {room_data.band_centre_hz[k]} Hz')\n",
    "\n",
    "        config_name = f'treble_data_grid_training_{room_data.band_centre_hz[k]}Hz_colorless_loss'\n",
    "        config_dict = load_and_validate_config(config_path + f'{config_name}.yml', DiffGFDNConfig)\n",
    "        sub_room_data = ThreeRoomDataset(\n",
    "            Path(config_dict.room_dataset_path).resolve(), config_dict)\n",
    "\n",
    "        sub_gfdn_room_data = deepcopy(sub_room_data)\n",
    "        sub_gfdn_room_data.update_receiver_pos(rec_pos_list)\n",
    "\n",
    "        config_dict = config_dict.model_copy(\n",
    "            update={\"num_groups\": sub_room_data.num_rooms})\n",
    "        trainer_config = config_dict.trainer_config\n",
    "\n",
    "        # force the trainer config device to be CPU\n",
    "        if trainer_config.device != 'cpu':\n",
    "            trainer_config = trainer_config.model_copy(\n",
    "                update={\"device\": 'cpu'})\n",
    "\n",
    "        # prepare the training and validation data for DiffGFDN\n",
    "        train_dataset, _ = load_dataset(\n",
    "            sub_gfdn_room_data,\n",
    "            trainer_config.device,\n",
    "            train_valid_split_ratio=1.0,\n",
    "            batch_size=trainer_config.batch_size,\n",
    "            shuffle=False)\n",
    "\n",
    "        if config_dict.colorless_fdn_config.use_colorless_prototype:\n",
    "            colorless_fdn_params = get_colorless_fdn_params(config_dict)\n",
    "        else:\n",
    "            colorless_fdn_params = None\n",
    "\n",
    "        # initialise the model\n",
    "        model = DiffGFDNVarReceiverPos(\n",
    "            config_dict.sample_rate,\n",
    "            config_dict.num_groups,\n",
    "            config_dict.delay_length_samps,\n",
    "            trainer_config.device,\n",
    "            config_dict.feedback_loop_config,\n",
    "            config_dict.output_filter_config,\n",
    "            use_absorption_filters=config_dict.decay_filter_config.use_absorption_filters,\n",
    "            common_decay_times=sub_gfdn_room_data.common_decay_times if\n",
    "                               config_dict.decay_filter_config.initialise_with_opt_values else None,\n",
    "            learn_common_decay_times=config_dict.decay_filter_config.learn_common_decay_times,\n",
    "            use_colorless_loss=trainer_config.use_colorless_loss,\n",
    "            colorless_fdn_params=colorless_fdn_params)\n",
    "\n",
    "        checkpoint_dir = Path(trainer_config.train_dir +\n",
    "                              'checkpoints/').resolve()\n",
    "\n",
    "        # load the trained weights for the particular epoch\n",
    "        checkpoint = torch.load(\n",
    "            f'{checkpoint_dir}/model_e{trainer_config.max_epochs-1}.pt',\n",
    "            weights_only=True,\n",
    "            map_location=torch.device('cpu'))\n",
    "        # Load the trained model state\n",
    "        model.load_state_dict(checkpoint)\n",
    "        # in eval mode, no gradients are calculated\n",
    "        model.eval()\n",
    "\n",
    "        # loop through all positions\n",
    "        for data in train_dataset:\n",
    "            position = data['listener_position'].detach().cpu().numpy()\n",
    "\n",
    "            if model.use_colorless_loss:\n",
    "                _, _, h = get_response(data, model)\n",
    "            else:\n",
    "                _, h = get_response(data, model)\n",
    "\n",
    "            # loop over all positions for a particular frequency band and add it to a dataframe\n",
    "            for num_pos in range(position.shape[0]):\n",
    "                cur_rir = h[num_pos, :].detach().cpu().numpy()\n",
    "                cur_rir_filtered = fftconvolve(\n",
    "                    cur_rir,\n",
    "                    subband_filters.coefficients[k, :],\n",
    "                    mode='same')\n",
    "\n",
    "                # position should be saved as tuple because numpy array is unhashable\n",
    "                new_row = pd.DataFrame({\n",
    "                    'frequency': [room_data.band_centre_hz[k]],\n",
    "                    'position':\n",
    "                    [(position[num_pos, 0], position[num_pos,\n",
    "                                                     1], position[num_pos,\n",
    "                                                                  2])],\n",
    "                    'filtered_time_samples': [cur_rir_filtered],\n",
    "                })\n",
    "                synth_subband_rirs = pd.concat(\n",
    "                    [synth_subband_rirs, new_row], ignore_index=True)\n",
    "\n",
    "    synth_rirs = synth_subband_rirs.groupby('position').apply(sum_arrays)\n",
    "\n",
    "    # Convert to DataFrame if needed\n",
    "    synth_rirs_df = synth_rirs.reset_index()\n",
    "    synth_rirs_df.columns = ['position', 'filtered_time_samples']\n",
    "\n",
    "    subband_gfdn_room_data = deepcopy(room_data)\n",
    "\n",
    "    \n",
    "    subband_gfdn_room_data.update_receiver_pos(np.array(synth_rirs_df['position'].to_list()))\n",
    "    subband_gfdn_room_data.update_rirs(np.vstack(synth_rirs_df['filtered_time_samples']))\n",
    "\n",
    "     # Save to a file\n",
    "    with open(subband_gfdn_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(subband_gfdn_room_data, f)\n",
    "\n",
    "else:\n",
    "    with open(subband_gfdn_pickle_path, \"rb\") as f:\n",
    "        subband_gfdn_room_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# some plotting to investigate\n",
    "rec_pos_idx = 34\n",
    "dist = np.linalg.norm(subband_gfdn_room_data.receiver_position[rec_pos_idx, :] - room_data.receiver_position, axis=-1)\n",
    "ref_rir_idx = np.argmin(dist, axis=0)\n",
    "plt.plot(subband_gfdn_room_data.late_rirs[rec_pos_idx, :])\n",
    "plt.plot(room_data.late_rirs[ref_rir_idx, :])\n",
    "\n",
    "S_true, freqs, time_frames = get_stft_torch(torch.tensor(room_data.late_rirs[rec_pos_idx, :]), \n",
    "                                       room_data.sample_rate, win_size=2**12, hop_size=2**11, nfft=2**12)\n",
    "S, freqs, time_frames = get_stft_torch(torch.tensor(subband_gfdn_room_data.late_rirs[rec_pos_idx, :]), \n",
    "                                       room_data.sample_rate, win_size=2**12, hop_size=2**11, nfft=2**12)\n",
    "\n",
    "plot_spectrogram(db(torch.abs(S_true)), freqs, time_frames, title='Ref RIR', log_freq_axis=True)\n",
    "plot_spectrogram(db(torch.abs(S)), freqs, time_frames, title='Subband DiffGFDN', log_freq_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Animate the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "reload(src.sound_examples)\n",
    "from src.sound_examples import dynamic_rendering_moving_receiver\n",
    "\n",
    "update_ms = 250 #should be a factor of 1s\n",
    "\n",
    "dynamic_renderer = dynamic_rendering_moving_receiver(room_data, rec_pos_list, speech_app, update_ms=update_ms)\n",
    "ani_save_path = Path(f'{fig_path}/sound_examples/treble_data').resolve()\n",
    "dynamic_renderer.animate_moving_listener(ani_save_path)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "ref_output = dynamic_renderer.filter_overlap_add()\n",
    "save_path = Path(f'{audio_path}/reference_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, ref_output, room_data.sample_rate) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_reference_{sig_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fading convolution with CS RIRs\n",
    "dynamic_renderer = dynamic_rendering_moving_receiver(cs_room_data, rec_pos_list, speech_app, update_ms=update_ms)\n",
    "cs_output = dynamic_renderer.filter_overlap_add()\n",
    "save_path = Path(f'{audio_path}/cs_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, cs_output, room_data.sample_rate) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_cs_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fading convolution with fullband GFDN RIRs\n",
    "dynamic_renderer = dynamic_rendering_moving_receiver(full_gfdn_room_data, rec_pos_list, speech_app, update_ms=update_ms)\n",
    "full_gfdn_output = dynamic_renderer.filter_overlap_add()\n",
    "save_path = Path(f'{audio_path}/fullband_gfdn_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, full_gfdn_output, room_data.sample_rate) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_fullband_DiffGFDN_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fading convolution with subband GFDN RIRs\n",
    "dynamic_renderer = dynamic_rendering_moving_receiver(subband_gfdn_room_data, rec_pos_list, speech_app, update_ms=update_ms)\n",
    "sub_gfdn_output = dynamic_renderer.filter_overlap_add()\n",
    "save_path = Path(f'{audio_path}/subband_gfdn_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, sub_gfdn_output, room_data.sample_rate) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', \n",
    "                                             f'{ani_save_path}_subband_DiffGFDN_{sig_type}_colorless_prototype')\n",
    "del dynamic_renderer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
