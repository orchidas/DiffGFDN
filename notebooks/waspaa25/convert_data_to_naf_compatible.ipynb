{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from typing import List\n",
    "from scipy.signal import stft\n",
    "from scipy.fft import rfft, irfft\n",
    "import spaudiopy as spa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('../..')\n",
    "from spatial_sampling.dataloader import parse_room_data, SpatialRoomDataset, load_dataset\n",
    "from spatial_sampling.config import SpatialSamplingConfig\n",
    "from diff_gfdn.utils import db\n",
    "from src.sofa_parser import HRIRSOFAReader, SRIRSOFAWriter, convert_srir_to_brir\n",
    "from src.run_model import load_and_validate_config\n",
    "from src.dataclass import NAFDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Notebook to convert coupled room dataset to NAF compatible\n",
    "\n",
    "NAF takes in BRIRs at spatial locations in the room for head orientations [0, 90, 180, 270]. To train NAF with different subsets of receivers, like we do in the WASPAA paper, we create different training and inference dataset containing SRIRs at different locations, and then convert them to BRIRs for the 4 given orientations. Finally we save them in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'output/spatial_sampling/sound_examples'\n",
    "room_data_pkl_path = Path('resources/Georg_3room_FDTD/srirs_spatial.pkl').resolve()\n",
    "config_path = Path('data/config/spatial_sampling/').resolve()\n",
    "save_path = Path('resources/Georg_3room_FDTD').resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Get the true room dataset for different grid spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = f'{config_path}/treble_data_grid_training_1000Hz_directional_spatial_sampling_test.yml'\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       SpatialSamplingConfig)\n",
    "hrtf_path = Path('resources/HRTF/48kHz/KEMAR_Knowl_EarSim_SmallEars_FreeFieldComp_48kHz.sofa')\n",
    "\n",
    "# get the original dataset\n",
    "room_data = parse_room_data(room_data_pkl_path)\n",
    "\n",
    "# get the HRTF\n",
    "hrtf_reader = HRIRSOFAReader(hrtf_path)\n",
    "\n",
    "if hrtf_reader.fs != room_data.sample_rate:\n",
    "    logger.info(\n",
    "            f\"Resampling HRTFs to {room_data.sample_rate:.0f} Hz\")\n",
    "    hrtf_reader.resample_hrirs(room_data.sample_rate)\n",
    "\n",
    "plt.figure()\n",
    "hrir_sh = hrtf_reader.get_spherical_harmonic_representation(2)\n",
    "print(hrir_sh.shape)\n",
    "plt.plot(hrir_sh[0,...].T)  # left ear\n",
    "plt.title(\"Raw SH HRIRs\")\n",
    "\n",
    "plt.figure()\n",
    "hrirs = hrtf_reader.ir_data\n",
    "print(hrirs.shape)\n",
    "plt.plot(hrirs[0, ...].T)  # left ear\n",
    "plt.title(\"Raw HRIRs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get train dataset for different grid spacings\n",
    "grid_resolution_m = np.arange(config_dict.num_grid_spacing, 0,\n",
    "                                  -1) * room_data.grid_spacing_m\n",
    "head_orientations = np.zeros((4, 2))\n",
    "# these are the only directions in NAF\n",
    "head_orientations[:, 0] = np.array([0, 90, 180, 270])\n",
    "\n",
    "for k in [1]:\n",
    "    logger.info(f'Creatng NAF dataset for grid spacing = {np.round(grid_resolution_m[k], 1)}m')\n",
    "\n",
    "    pkl_path = f'{save_path}/naf_dataset_grid_spacing={grid_resolution_m[k]:.1f}m.pkl'\n",
    "\n",
    "    if not os.path.exists(pkl_path):\n",
    "        all_train_rec_pos = []\n",
    "        all_train_srir = []\n",
    "        all_valid_rec_pos = []\n",
    "        all_valid_srir = []\n",
    "        \n",
    "        # prepare the training and validation data for DiffGFDN\n",
    "        train_dataset, valid_dataset, dataset_ref = load_dataset(\n",
    "            room_data,\n",
    "            config_dict.device,\n",
    "            grid_resolution_m=np.round(grid_resolution_m[k], 1),\n",
    "            network_type=config_dict.network_type,\n",
    "            batch_size=config_dict.batch_size)\n",
    "    \n",
    "        logger.info(\"Creating training BRIRs\")\n",
    "        # training data\n",
    "        for data in train_dataset:\n",
    "            cur_list_pos = data['listener_position'].detach().cpu().numpy()\n",
    "            all_train_rec_pos.append(cur_list_pos)\n",
    "            indx = room_data.find_rec_idx_in_room_dataset(cur_list_pos)\n",
    "            cur_srir = room_data.rirs[indx, ...]\n",
    "            all_train_srir.append(cur_srir)\n",
    "    \n",
    "        train_srir = np.vstack(all_train_srir)\n",
    "        train_pos = np.vstack(all_train_rec_pos)\n",
    "        train_brirs = convert_srir_to_brir(train_srir, hrtf_reader, head_orientations)\n",
    "    \n",
    "        logger.info(\"Creating inference BRIRs\")\n",
    "        # inference data\n",
    "        if grid_resolution_m[k] != room_data.grid_spacing_m:\n",
    "            for data in valid_dataset:\n",
    "                cur_list_pos = data['listener_position'].detach().cpu().numpy()\n",
    "                all_valid_rec_pos.append(cur_list_pos)\n",
    "                indx = room_data.find_rec_idx_in_room_dataset(cur_list_pos)\n",
    "                cur_srir = room_data.rirs[indx, ...]\n",
    "                all_valid_srir.append(cur_srir)\n",
    "        \n",
    "            valid_srir = np.vstack(all_valid_srir)\n",
    "            valid_pos = np.vstack(all_valid_rec_pos)\n",
    "            valid_brirs = convert_srir_to_brir(valid_srir, hrtf_reader, head_orientations)\n",
    "            num_valid_receivers = valid_pos.shape[0]\n",
    "        else:\n",
    "            valid_srir = None\n",
    "            valid_pos = None\n",
    "            valid_brirs = None\n",
    "            num_valid_receivers = None\n",
    "        \n",
    "    \n",
    "        logger.info(\"Creating NAF dataset\")\n",
    "        naf_dataset = NAFDataset(num_train_receivers = train_pos.shape[0],\n",
    "                                 num_infer_receivers = num_valid_receivers,\n",
    "                                 train_receiver_pos = train_pos,\n",
    "                                 infer_receiver_pos = valid_pos,\n",
    "                                 train_brirs = train_brirs,\n",
    "                                 infer_brirs = valid_brirs,\n",
    "                                 orientation = head_orientations[0, :],                      \n",
    "                                )\n",
    "    \n",
    "        # Step 3: Save the instance to a pickle file\n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(naf_dataset, f)\n",
    "    else:\n",
    "        logger.info(\"File already exists!\")\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            naf_dataset = pickle.load(f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Plot a BRIR for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# srir = train_srir[52, 0, ...]\n",
    "# plt.plot(srir)\n",
    "# plt.title(\"Example SRIR\")\n",
    "# plt.show()\n",
    "\n",
    "brir = naf_dataset.train_brirs[52, 0, ...]\n",
    "plt.plot(brir)\n",
    "plt.title(\"Example BRIR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Plot the STFT and IF that is used by NAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, S = stft(brir, fs=room_data.sample_rate, window='hann', \n",
    "               nperseg=256, noverlap=None, nfft=2**10, return_onesided=True, axis = 0)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(211)\n",
    "plt.pcolormesh(t, f, db(np.squeeze(S[:, 0, :])), shading='gouraud', cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Freq (Hz)')\n",
    "plt.ylim([20, 16000])\n",
    "plt.title('Spectrogram for left ear')\n",
    "plt.colorbar(label='Magnitude(dB)')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.pcolormesh(t, f, db(np.squeeze(S[:, 1, :])), shading='gouraud', cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Freq(Hz)')\n",
    "plt.ylim([20, 16000])\n",
    "plt.title('Spectrogram for right ear')\n",
    "plt.colorbar(label='Magnitude(dB)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.shape, S.shape)\n",
    "phase = np.unwrap(np.angle(S))\n",
    "instant_freq = np.diff(phase, n=1, axis=-1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(211)\n",
    "plt.pcolormesh(t[:-1], f, np.squeeze(instant_freq[:, 0, :]), shading='gouraud', cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Freq (Hz)')\n",
    "plt.ylim([20, 16000])\n",
    "plt.title('Instantaneous frequency left ear')\n",
    "plt.colorbar(label='rad/s')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.pcolormesh(t[:-1], f, np.squeeze(instant_freq[:, 1, :]), shading='gouraud', cmap='viridis')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Freq(Hz)')\n",
    "plt.ylim([20, 16000])\n",
    "plt.title('Instantaneous frequency right ear')\n",
    "plt.colorbar(label='rad/s')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
