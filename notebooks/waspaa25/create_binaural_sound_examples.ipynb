{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This notebook generates binaural sound examples for the common slopes amplitudes interpolation problem. \n",
    "First the soundfield at octave bands is generated by getting the learned amplitudes from the DNN in octave bands. Then an ambisonics RIR is\n",
    "reconstructed from the learned amplitudes using white noise shaping.\n",
    "\n",
    "Simultaneously, an HRTF dataset is loaded and converted to the ambisonics domain. The ambisonics RIRs are first rotated, according to the head orientation and then convolved with the HRTFs' SH representation. This rotated soundfield is then convolved with the input mono signal to get the binauralised output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import pyfar as pf\n",
    "import librosa\n",
    "import pickle\n",
    "import IPython\n",
    "from loguru import logger\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir('../..')\n",
    "\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "from spatial_sampling.dataloader import SpatialThreeRoomDataset, SpatialRoomDataset\n",
    "from diff_gfdn.utils import ms_to_samps\n",
    "\n",
    "from src.sofa_parser import HRIRSOFAReader, SRIRSOFAWriter, save_to_sofa\n",
    "from src.sound_examples import binaural_dynamic_rendering, add_direct_and_early_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'output/spatial_sampling/sound_examples'\n",
    "audio_path = 'audio/sound_examples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Create a trajectory of a listener moving across the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# along x axis between three rooms\n",
    "start_pos_x, start_pos_y = (0.5, 3.5)\n",
    "end_pos_x, end_pos_y = (9, 3.5)\n",
    "num_pos = 50\n",
    "head_orientation_az = np.deg2rad(np.linspace(200, 30, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "\n",
    "rec_pos_list = np.zeros((num_pos, 3))\n",
    "rec_pos_list[:, 0] = linear_trajectory_x\n",
    "rec_pos_list[:, 1] = linear_trajectory_y\n",
    "rec_pos_list[:, 2] = linear_trajectory_z\n",
    "orientation_list = np.zeros((num_pos, 2))\n",
    "orientation_list[:, 0] = head_orientation_az\n",
    "orientation_list[:, 1] = head_orientation_el\n",
    "\n",
    "# along y-axis between rooms 2 and 3\n",
    "start_pos_x, start_pos_y = (9.1, 3.5)\n",
    "end_pos_x, end_pos_y = (9.0, 12.0)\n",
    "num_pos = 68\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "head_orientation_az = np.deg2rad(np.linspace(30, 150, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "rec_pos_list = np.vstack((rec_pos_list, np.vstack((linear_trajectory_x, linear_trajectory_y, linear_trajectory_z)).T))\n",
    "head_orientation_list = np.vstack((orientation_list, np.vstack((head_orientation_az, head_orientation_el)).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Get the true room dataset with its corresponding ambisonics RIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_data_pkl_path = Path('resources/Georg_3room_FDTD/srirs_spatial.pkl').resolve()\n",
    "config_path = Path('data/config/spatial_sampling/').resolve()\n",
    "\n",
    "# get the original dataset\n",
    "true_cs_room_data = SpatialThreeRoomDataset(room_data_pkl_path)\n",
    "\n",
    "save_path = Path('resources/SOFA files/true_ambi_srirs.sofa').resolve()\n",
    "save_to_sofa(deepcopy(true_cs_room_data), save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Get the mono, dry stimulus and resample it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_type = 'speech'\n",
    "\n",
    "speech_data = pf.signals.files.drums() if sig_type == 'drums' else pf.signals.files.speech()\n",
    "speech = np.squeeze(speech_data.time)\n",
    "fs = speech_data.sampling_rate\n",
    "new_fs = int(true_cs_room_data.sample_rate)\n",
    "\n",
    "if fs != new_fs:\n",
    "    speech = librosa.resample(speech, orig_sr = fs, target_sr = new_fs)\n",
    "\n",
    "# add some silence at the end\n",
    "silence = np.zeros(ms_to_samps(500, new_fs))\n",
    "speech_app = np.concatenate((speech, silence))\n",
    "                   \n",
    "save_path = Path(f'{audio_path}/stimulus/{sig_type}.wav').resolve()\n",
    "sf.write(save_path, speech_app, new_fs)\n",
    "IPython.display.Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Load the HRTF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sofa_parser import HRIRSOFAReader\n",
    "\n",
    "hrtf_path = Path('resources/HRTF/48kHz/KEMAR_Knowl_EarSim_SmallEars_FreeFieldComp_48kHz.sofa')\n",
    "hrtf_reader = HRIRSOFAReader(hrtf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Create a sound examples object for the reference RIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sound_examples import binaural_dynamic_rendering\n",
    "\n",
    "update_ms = 250 #should be a factor of 1s\n",
    "ani_save_path = Path(f'{out_path}/treble_data_binaural').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(true_cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "dynamic_renderer.animate_moving_listener(ani_save_path)\n",
    "save_path = Path(f'{out_path}/extended_stimulus_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, dynamic_renderer.extended_stimulus, int(true_cs_room_data.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a subset of the receiver positions used to create the listening examples as SOFA file\n",
    "subset_true_cs_room_data = deepcopy(true_cs_room_data)\n",
    "subset_rirs = true_cs_room_data.rirs[dynamic_renderer.rec_idxs, ...]\n",
    "subset_true_cs_room_data.update_rirs(subset_rirs)\n",
    "subset_true_cs_room_data.update_receiver_pos(rec_pos_list)\n",
    "\n",
    "save_path = Path('resources/SOFA files/true_ambi_srirs_trajectory.sofa').resolve()\n",
    "save_to_sofa(deepcopy(subset_true_cs_room_data), save_path)\n",
    "\n",
    "save_path = Path('resources/SOFA files/true_ambi_srirs_late_trajectory.sofa').resolve()\n",
    "save_to_sofa(deepcopy(subset_true_cs_room_data), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fading convolution with the reference set of RIRs\n",
    "ref_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "ref_output_norm = dynamic_renderer.normalise_loudness(ref_output, true_cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_reference_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, ref_output_norm, int(true_cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_reference_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Get the room dataset using the common slopes directional amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import spatial_sampling\n",
    "reload(spatial_sampling.inference)\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "\n",
    "output_pkl_path = Path('output/spatial_sampling/grid_rir_treble_cs_ambi_rirs.pkl').resolve()\n",
    "\n",
    "# get predicted output from the trained models\n",
    "if not os.path.exists(output_pkl_path):\n",
    "    cs_room_data = get_ambisonic_rirs(rec_pos_list, true_cs_room_data, \n",
    "                                      use_trained_model=False, output_pkl_path=output_pkl_path)\n",
    "else:\n",
    "    with open(output_pkl_path, \"rb\") as f:\n",
    "        cs_room_data = pickle.load(f)\n",
    "\n",
    "save_path = Path('resources/SOFA files/cs_predicted_ambi_srirs_late.sofa').resolve()\n",
    "save_to_sofa(deepcopy(cs_room_data), save_path)\n",
    "\n",
    "save_path = Path('resources/SOFA files/cs_predicted_ambi_srirs.sofa').resolve()\n",
    "full_rirs = add_direct_and_early_path(true_cs_room_data, cs_room_data)\n",
    "\n",
    "cs_room_data.update_rirs(full_rirs)\n",
    "save_to_sofa(deepcopy(cs_room_data), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Create sound example with CS SRIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "cs_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "cs_output_norm = dynamic_renderer.normalise_loudness(cs_output, cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_cs_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, cs_output_norm, int(cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_cs_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Get the room dataset with the predicted amplitudes from the DNN with its corresponding ambisonics RIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import spatial_sampling\n",
    "reload(spatial_sampling.inference)\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "\n",
    "grid_res = 0.9\n",
    "output_pkl_path = Path(f'output/spatial_sampling/grid_rir_treble_mlp_ambi_rirs_grid_res={grid_res:.1f}.pkl').resolve()\n",
    "\n",
    "# get predicted output from the trained models\n",
    "if not os.path.exists(output_pkl_path):\n",
    "    pred_cs_room_data = get_ambisonic_rirs(rec_pos_list,true_cs_room_data, use_trained_model=True, \n",
    "                                           config_path=config_path, grid_resolution_m=grid_res, output_pkl_path=output_pkl_path)\n",
    "else:\n",
    "    with open(output_pkl_path, \"rb\") as f:\n",
    "        pred_cs_room_data = pickle.load(f)\n",
    "\n",
    "save_path = Path(f'resources/SOFA files/mlp_predicted_ambi_srirs_late_grid_spacing={grid_res:.1f}m.sofa').resolve()\n",
    "save_to_sofa(deepcopy(pred_cs_room_data), save_path)\n",
    "\n",
    "save_path = Path(f'resources/SOFA files/mlp_predicted_ambi_srirs_grid_spacing={grid_res:.1f}m.sofa').resolve()\n",
    "full_rirs = add_direct_and_early_path(true_cs_room_data, pred_cs_room_data)\n",
    "pred_cs_room_data.update_rirs(full_rirs)\n",
    "save_to_sofa(deepcopy(pred_cs_room_data), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Plot an RIR for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = ms_to_samps(50, true_cs_room_data.sample_rate)\n",
    "end_idx = int(2*true_cs_room_data.sample_rate)\n",
    "pos_num = 18\n",
    "chan_num = 2\n",
    "plt.plot(cs_room_data.rirs[pos_num, chan_num, start_idx:end_idx])\n",
    "plt.plot(pred_cs_room_data.rirs[pos_num, chan_num,start_idx:end_idx])\n",
    "# plt.plot(true_cs_room_data.rirs[pos_num, chan_num, start_idx:end_idx])\n",
    "plt.legend(['CS pred', 'MLP pred', 'Reference'])\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/reference_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, true_cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/cs_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/pred_cs_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, pred_cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Create sound example with MLP predicted SRIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(pred_cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "pred_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "pred_output_norm = dynamic_renderer.normalise_loudness(pred_output, pred_cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_mlp_grid_res={grid_res:.1f}_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, pred_output_norm, int(pred_cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', \n",
    "                                             f'{ani_save_path}_mlp_grid_res={grid_res:.1f}_{sig_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
