{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This notebook generates binaural sound examples for the common slopes amplitudes interpolation problem. \n",
    "First the soundfield at octave bands is generated by getting the learned amplitudes from the DNN in octave bands. Then an ambisonics RIR is\n",
    "reconstructed from the learned amplitudes using white noise shaping.\n",
    "\n",
    "Simultaneously, an HRTF dataset is loaded and converted to the ambisonics domain. The ambisonics RIRs are first rotated, according to the head orientation and then convolved with the HRTFs' SH representation. This rotated soundfield is then convolved with the input mono signal to get the binauralised output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import pyfar as pf\n",
    "import librosa\n",
    "import pickle\n",
    "import IPython\n",
    "from loguru import logger\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "from spatial_sampling.dataloader import parse_room_data, SpatialRoomDataset\n",
    "from diff_gfdn.utils import ms_to_samps\n",
    "\n",
    "from src.sofa_parser import HRIRSOFAReader, SRIRSOFAWriter\n",
    "from src.sound_examples import binaural_dynamic_rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'output/spatial_sampling/sound_examples'\n",
    "audio_path = 'audio/sound_examples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### Save the ambisonics SRIRs as a SOFA file for testing with SPARTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import src\n",
    "reload(src.sofa_parser)\n",
    "from src.sofa_parser import SRIRSOFAWriter\n",
    "\n",
    "def save_to_sofa(cs_room_data: SpatialRoomDataset, save_path: str, new_fs: float):\n",
    "    if not os.path.exists(save_path):\n",
    "        if 'late' in save_path.as_posix():\n",
    "            cs_room_data.early_late_split()\n",
    "            ir_len = cs_room_data.late_rirs.shape[-1]\n",
    "        else:\n",
    "            ir_len = cs_room_data.rir_length\n",
    "            \n",
    "        sofa_writer = SRIRSOFAWriter(cs_room_data.num_rec, \n",
    "                                     cs_room_data.ambi_order, \n",
    "                                     ir_len, \n",
    "                                     cs_room_data.sample_rate)\n",
    "    \n",
    "       \n",
    "        # source and receiver positions are flipped in SPARTA\n",
    "        sofa_writer.set_receiver_positions(cs_room_data.receiver_position)\n",
    "        sofa_writer.set_source_positions(cs_room_data.source_position)\n",
    "        if 'late' in save_path.as_posix():\n",
    "            sofa_writer.set_ir_data(cs_room_data.late_rirs)\n",
    "        else:\n",
    "            sofa_writer.set_ir_data(cs_room_data.rirs)\n",
    "        \n",
    "        sofa_writer.resample_srirs(new_fs)\n",
    "        sofa_writer.write_to_file(save_path)\n",
    "    else:\n",
    "        logger.info(\"SOFA file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Create a trajectory of a listener moving across the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# along x axis between three rooms\n",
    "start_pos_x, start_pos_y = (0.5, 3.5)\n",
    "end_pos_x, end_pos_y = (9, 3.5)\n",
    "num_pos = 50\n",
    "head_orientation_az = np.deg2rad(np.linspace(200, 30, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "\n",
    "rec_pos_list = np.zeros((num_pos, 3))\n",
    "rec_pos_list[:, 0] = linear_trajectory_x\n",
    "rec_pos_list[:, 1] = linear_trajectory_y\n",
    "rec_pos_list[:, 2] = linear_trajectory_z\n",
    "orientation_list = np.zeros((num_pos, 2))\n",
    "orientation_list[:, 0] = head_orientation_az\n",
    "orientation_list[:, 1] = head_orientation_el\n",
    "\n",
    "# along y-axis between rooms 2 and 3\n",
    "start_pos_x, start_pos_y = (9.1, 3.5)\n",
    "end_pos_x, end_pos_y = (9.0, 12.0)\n",
    "num_pos = 68\n",
    "\n",
    "linear_trajectory_x = np.linspace(start_pos_x, end_pos_x, num_pos)\n",
    "linear_trajectory_y = np.linspace(start_pos_y, end_pos_y, num_pos)\n",
    "linear_trajectory_z = 1.5 * np.ones(num_pos)\n",
    "head_orientation_az = np.deg2rad(np.linspace(30, 150, num_pos))\n",
    "head_orientation_el = np.deg2rad(np.zeros(num_pos))\n",
    "\n",
    "rec_pos_list = np.vstack((rec_pos_list, np.vstack((linear_trajectory_x, linear_trajectory_y, linear_trajectory_z)).T))\n",
    "head_orientation_list = np.vstack((orientation_list, np.vstack((head_orientation_az, head_orientation_el)).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Get the true room dataset with its corresponding ambisonics RIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_data_pkl_path = Path('resources/Georg_3room_FDTD/srirs_spatial.pkl').resolve()\n",
    "config_path = Path('data/config/spatial_sampling/').resolve()\n",
    "\n",
    "# get the original dataset\n",
    "true_cs_room_data = parse_room_data(room_data_pkl_path)\n",
    "new_fs = 48000\n",
    "\n",
    "save_path = Path('resources/SOFA files/true_ambi_srirs_late.sofa').resolve()\n",
    "save_to_sofa(deepcopy(true_cs_room_data), save_path, new_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Get the mono, dry stimulus and resample it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_type = 'speech'\n",
    "\n",
    "speech_data = pf.signals.files.drums() if sig_type == 'drums' else pf.signals.files.speech()\n",
    "speech = np.squeeze(speech_data.time)\n",
    "fs = speech_data.sampling_rate\n",
    "new_fs = int(true_cs_room_data.sample_rate)\n",
    "\n",
    "if fs != new_fs:\n",
    "    speech = librosa.resample(speech, orig_sr = fs, target_sr = new_fs)\n",
    "\n",
    "# add some silence at the end\n",
    "silence = np.zeros(ms_to_samps(500, new_fs))\n",
    "speech_app = np.concatenate((speech, silence))\n",
    "                   \n",
    "save_path = Path(f'{audio_path}/stimulus/{sig_type}.wav').resolve()\n",
    "sf.write(save_path, speech_app, new_fs)\n",
    "IPython.display.Audio(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Load the HRTF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sofa_parser import HRIRSOFAReader\n",
    "\n",
    "hrtf_path = Path('resources/HRTF/48kHz/KEMAR_Knowl_EarSim_SmallEars_FreeFieldComp_48kHz.sofa')\n",
    "hrtf_reader = HRIRSOFAReader(hrtf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Create a sound examples object for the reference RIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sound_examples import binaural_dynamic_rendering\n",
    "\n",
    "update_ms = 250 #should be a factor of 1s\n",
    "ani_save_path = Path(f'{out_path}/treble_data_binaural').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(true_cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "dynamic_renderer.animate_moving_listener(ani_save_path)\n",
    "save_path = Path(f'{out_path}/extended_stimulus_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, dynamic_renderer.extended_stimulus, int(true_cs_room_data.sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-fading convolution with the reference set of RIRs\n",
    "ref_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "ref_output_norm = dynamic_renderer.normalise_loudness(ref_output, true_cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_reference_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, ref_output_norm, int(true_cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_reference_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Get the room dataset using the common slopes directional amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import spatial_sampling\n",
    "reload(spatial_sampling.inference)\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "\n",
    "output_pkl_path = Path('output/spatial_sampling/grid_rir_treble_cs_ambi_rirs.pkl').resolve()\n",
    "\n",
    "# get predicted output from the trained models\n",
    "if not os.path.exists(output_pkl_path):\n",
    "    cs_room_data = get_ambisonic_rirs(rec_pos_list, output_pkl_path, true_cs_room_data, use_trained_model=False)\n",
    "else:\n",
    "    with open(output_pkl_path, \"rb\") as f:\n",
    "        cs_room_data = pickle.load(f)\n",
    "\n",
    "save_path = Path('resources/SOFA files/cs_predicted_ambi_srirs_late.sofa').resolve()\n",
    "save_to_sofa(deepcopy(cs_room_data), save_path, new_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Create sound example with CS SRIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "cs_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "cs_output_norm = dynamic_renderer.normalise_loudness(cs_output, cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_cs_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, cs_output_norm, int(cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', f'{ani_save_path}_cs_{sig_type}')\n",
    "del dynamic_renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Get the room dataset with the predicted amplitudes from the DNN with its corresponding ambisonics RIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import spatial_sampling\n",
    "reload(spatial_sampling.inference)\n",
    "from spatial_sampling.inference import get_ambisonic_rirs\n",
    "\n",
    "grid_res = 0.6\n",
    "output_pkl_path = Path(f'output/spatial_sampling/grid_rir_treble_mlp_ambi_rirs_grid_res={grid_res:.1f}.pkl').resolve()\n",
    "\n",
    "# get predicted output from the trained models\n",
    "if not os.path.exists(output_pkl_path):\n",
    "    pred_cs_room_data = get_ambisonic_rirs(rec_pos_list, output_pkl_path, true_cs_room_data, \n",
    "                                           use_trained_model=True, config_path=config_path, grid_resolution_m=grid_res)\n",
    "else:\n",
    "    with open(output_pkl_path, \"rb\") as f:\n",
    "        pred_cs_room_data = pickle.load(f)\n",
    "\n",
    "save_path = Path(f'resources/SOFA files/mlp_predicted_ambi_srirs_late_grid_spacing={grid_res:.1f}m.sofa').resolve()\n",
    "save_to_sofa(deepcopy(pred_cs_room_data), save_path, new_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Plot an RIR for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_idx = ms_to_samps(50, true_cs_room_data.sample_rate)\n",
    "end_idx = int(2*true_cs_room_data.sample_rate)\n",
    "pos_num = 18\n",
    "chan_num = 2\n",
    "plt.plot(cs_room_data.rirs[pos_num, chan_num, start_idx:end_idx])\n",
    "plt.plot(pred_cs_room_data.rirs[pos_num, chan_num,start_idx:end_idx])\n",
    "plt.plot(true_cs_room_data.rirs[pos_num, chan_num, start_idx:end_idx])\n",
    "plt.legend(['CS pred', 'MLP pred', 'Reference'])\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/reference_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, true_cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/cs_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)\n",
    "\n",
    "save_path = Path(f'{audio_path}/reference_ambi/pred_cs_ir_pos={pos_num}_chan={chan_num}.wav').resolve()\n",
    "sf.write(save_path, pred_cs_room_data.rirs[pos_num, chan_num, :], true_cs_room_data.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Create sound example with MLP predicted SRIRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_renderer = binaural_dynamic_rendering(pred_cs_room_data, \n",
    "                                             rec_pos_list, \n",
    "                                             head_orientation_list, \n",
    "                                             speech_app, \n",
    "                                             hrtf_reader, \n",
    "                                             update_ms=update_ms)\n",
    "\n",
    "# cross-fading convolution with the reference set of RIRs\n",
    "pred_output = dynamic_renderer.binaural_filter_overlap_add()\n",
    "\n",
    "pred_output_norm = dynamic_renderer.normalise_loudness(pred_output, pred_cs_room_data.sample_rate, db_lufs=-24)\n",
    "save_path = Path(f'{out_path}/binaural_mlp_grid_res={grid_res:.1f}_moving_listener_{sig_type}.wav').resolve()\n",
    "sf.write(save_path, pred_output_norm, int(pred_cs_room_data.sample_rate)) \n",
    "IPython.display.Audio(save_path)\n",
    "\n",
    "dynamic_renderer.combine_animation_and_sound(f'{ani_save_path}_moving_listener.mp4', f'{save_path}', \n",
    "                                             f'{ani_save_path}_mlp_grid_res={grid_res:.1f}_{sig_type}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
