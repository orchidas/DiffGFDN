{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Load dataset and save IRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from loguru import logger\n",
    "\n",
    "os.chdir('..')  # This changes the working directory to DiffGFDN\n",
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset\n",
    "from diff_gfdn.config.config import DiffGFDNConfig, CouplingMatrixType\n",
    "from diff_gfdn.model import DiffGFDNVarReceiverPos\n",
    "from diff_gfdn.utils import get_response, db, ms_to_samps, is_unitary, is_paraunitary, normalised_echo_density\n",
    "from diff_gfdn.losses import get_stft_torch, get_edr_from_stft\n",
    "from diff_gfdn.plot import (plot_polynomial_matrix_magnitude_response, \n",
    "                            plot_edr, plot_subband_amplitudes, plot_subband_edc,\n",
    "                            plot_learned_svf_response, plot_amps_in_space, plot_magnitude_response)\n",
    "from diff_gfdn.analysis import get_amps_for_rir\n",
    "from src.run_model import load_and_validate_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Read config files and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/'\n",
    "config_name = 'treble_data_grid_training_full_band_colorless_loss'\n",
    "config_file = config_path + f'{config_name}.yml'\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)\n",
    "\n",
    "\n",
    "if \"3room_FDTD\" in config_dict.room_dataset_path:\n",
    "    room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve(), config_dict)\n",
    "else:\n",
    "    room_data = convert_common_slopes_rir_to_room_dataset(config_dict.room_dataset_path, \n",
    "                                                          num_freq_bins=config_dict.trainer_config.num_freq_bins,\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of groups to the config dictionary\n",
    "config_dict = config_dict.model_copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "if config_dict.sample_rate != room_data.sample_rate:\n",
    "    logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "    config_dict.sample_rate = sample_rate\n",
    "\n",
    "# get the training config\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# force the trainer config device to be CPU\n",
    "if trainer_config.device != 'cpu':\n",
    "    trainer_config = trainer_config.model_copy(update={\"device\": 'cpu'})\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, train_valid_split_ratio=1.0,\n",
    "    batch_size=trainer_config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Check output data and compare with true IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the model\n",
    "model = DiffGFDNVarReceiverPos(room_data.sample_rate, \n",
    "                               room_data.num_rooms,\n",
    "                               config_dict.delay_length_samps,\n",
    "                               trainer_config.device, \n",
    "                               config_dict.feedback_loop_config,\n",
    "                               config_dict.output_filter_config,\n",
    "                               config_dict.decay_filter_config.use_absorption_filters,\n",
    "                               common_decay_times=room_data.common_decay_times,\n",
    "                               band_centre_hz=room_data.band_centre_hz,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory  = Path(\"audio/\")\n",
    "fig_path = Path(\"figures\").resolve()\n",
    "checkpoint_dir = Path(trainer_config.train_dir + 'checkpoints/').resolve()\n",
    "max_epochs = trainer_config.max_epochs\n",
    "save_ir_dir = Path(trainer_config.ir_dir).resolve() \n",
    "plot_ir = False\n",
    "# investigate outputs at this position\n",
    "pos_to_investigate = [9.3, 6.60, 1.50] #[2.0, 6.8, 1.5]\n",
    "desired_filename = f'ir_({pos_to_investigate[0]:.2f}, {pos_to_investigate[1]:.2f}, {pos_to_investigate[2]:.2f}).wav'\n",
    "\n",
    "# find amplitudes corresponding to the receiver position\n",
    "rec_pos_idx = np.where(\n",
    "    np.all(np.round(room_data.receiver_position, 2) == pos_to_investigate, axis=1))[0]\n",
    "amplitudes = room_data.amplitudes[rec_pos_idx, ...]\n",
    "h_true = np.squeeze(room_data.rirs[rec_pos_idx, :])\n",
    "\n",
    "all_pos = []\n",
    "all_rirs = []\n",
    "h_approx_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_biquad_coeffs = []\n",
    "for epoch in [-1,max_epochs-1]:\n",
    "    # load the trained weights for the particular epoch\n",
    "    checkpoint = torch.load(f'{checkpoint_dir}/model_e{epoch}.pt', weights_only=True, map_location=torch.device('cpu'))\n",
    "    # Load the trained model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    \n",
    "    for data in train_dataset:\n",
    "        position = data['listener_position']\n",
    "        H, h = get_response(data, model)\n",
    "        \n",
    "        for num_pos in range(position.shape[0]):\n",
    "            filename = f'ir_({position[num_pos,0]:.2f}, {position[num_pos, 1]:.2f}, {position[num_pos, 2]:.2f}).wav'\n",
    "\n",
    "            # collate all RIRs at all positions\n",
    "            if epoch == max_epochs - 1:\n",
    "                all_pos.append(position[num_pos])\n",
    "                all_rirs.append(h[num_pos, :])\n",
    "\n",
    "            if filename == desired_filename:\n",
    "                # get parameter dictionary used in inferencing\n",
    "                inf_param_dict = model.get_param_dict_inference(data)\n",
    "\n",
    "                # get the biquad coefficients for this position\n",
    "                h_approx_list.append(h[num_pos,:])\n",
    "                output_biquad_coeffs.append(inf_param_dict['output_biquad_coeffs'][num_pos])\n",
    "\n",
    "                if plot_ir:\n",
    "                    # plot the EDRs of the true and estimated\n",
    "                    plot_edr(torch.tensor(h_true), model.sample_rate, title=f'True RIR EDR, epoch={epoch}', \n",
    "                             save_path=f'{fig_path}/true_edr_{filename}_{config_name}_epoch={epoch}.png')\n",
    "    \n",
    "                    plot_edr(h[num_pos, :], model.sample_rate, title=f'Estimated RIR EDR, epoch={epoch}', \n",
    "                             save_path=f'{fig_path}/approx_edr_{filename}_{config_name}_epoch={epoch}.png')\n",
    "            \n",
    "                    plt.plot(torch.stack((torch.tensor(h_true), h[num_pos, :len(h_true)]), dim=-1))\n",
    "                    plt.xlim([0, int(1.5 * model.sample_rate)])\n",
    "                    plt.savefig(f'{fig_path}/ir_compare_{filename}_{config_name}_epoch={epoch}.png')\n",
    "                    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Plot subband EDC as a function of epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.plot import plot_subband_edc\n",
    "plot_subband_edc(h_true, h_approx_list, config_dict.sample_rate, room_data.band_centre_hz, pos_to_investigate, \n",
    "                 save_path=f'{fig_path}/compare_synth_edf_{pos_to_investigate}_{config_name}.png', \n",
    "                 epoch_numbers=[-1, max_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Plot the output filter response for the position under investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.plot import plot_learned_svf_response\n",
    "save_path = f'{fig_path}/{config_name}'\n",
    "plot_learned_svf_response(room_data.num_rooms, int(room_data.sample_rate), \n",
    "                          output_biquad_coeffs, pos_to_investigate, save_path=save_path, epoch_numbers=[-1, max_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Investigate the modes of the receiver filters to see how they affect the modes of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from scipy.signal import sos2zpk, zpk2tf, residue\n",
    "\n",
    "def compute_modes_from_sos(sos: NDArray) -> Tuple[ArrayLike, ArrayLike]:\n",
    "    \"\"\"\n",
    "    Compute the modes (poles and residues) using partial fraction expansion from SOS representation.\n",
    "\n",
    "    Args:\n",
    "        sos (array-like): Second-order sections (SOS) filter representation.\n",
    "\n",
    "    Returns:\n",
    "        poles (np.ndarray): Poles of the system.\n",
    "        residues (np.ndarray): Residues corresponding to each pole.\n",
    "    \"\"\"\n",
    "    # Convert SOS to zero-pole-gain form\n",
    "    z, p, k = sos2zpk(sos)\n",
    "\n",
    "    # Convert to transfer function form\n",
    "    b, a = zpk2tf(z, p, k)  # Get numerator and denominator polynomials\n",
    "\n",
    "    # Perform partial fraction expansion\n",
    "    residues, poles, _ = residue(b, a)\n",
    "\n",
    "    return poles, residues\n",
    "    \n",
    "def zp_to_modes(residues: List, poles: List, fs: float) -> Dict:\n",
    "    radius = np.abs(poles)\n",
    "    angles = np.angle(poles)\n",
    "    assert (radius < np.ones_like(radius)).all()\n",
    "    modal_params = {}\n",
    "    # in Hz\n",
    "    modal_params['freqs'] = (angles * fs) / (2*np.pi)\n",
    "    # T60 in ms\n",
    "    modal_params['decay'] = (6.91 / (-np.log(radius) * fs)) * 1e3\n",
    "    # in db\n",
    "    modal_params['amps'] = db(residues)\n",
    "    return modal_params\n",
    "    \n",
    "for idx, epoch in zip([1], [max_epochs-1]):\n",
    "    cur_biquad_coeffs = output_biquad_coeffs[idx]\n",
    "    for k in range(room_data.num_rooms):\n",
    "        cur_sos = cur_biquad_coeffs[k]\n",
    "        poles, residues = compute_modes_from_sos(cur_sos)\n",
    "        modal_params = zp_to_modes(residues, poles, room_data.sample_rate)\n",
    "        logger.info(f'Modal parameters for receiver filters in group {k+1} are: ')\n",
    "        logger.info(f'{np.round(modal_params['freqs'], 3)} Hz, \\n {np.round(modal_params['decay'], 3)} ms, \\n {np.round(modal_params['amps'], 3)} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Plot actual and achieved subband amplitudes for the position under investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual RIR levels\n",
    "reload(diff_gfdn.plot)\n",
    "from diff_gfdn.plot import plot_subband_amplitudes\n",
    "final_approx_rir = h_approx_list[-1].clone().detach()\n",
    "plot_subband_amplitudes(h_true, final_approx_rir, room_data.sample_rate, \n",
    "                        config_dict.num_groups, amplitudes, np.squeeze(room_data.common_decay_times), room_data.band_centre_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Get the final trained parameters and investigate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "param_path = Path(trainer_config.train_dir + '/parameters_opt.mat').resolve()\n",
    "opt_params = loadmat(param_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Observe the individual mixing matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import diff_gfdn\n",
    "reload(diff_gfdn.utils)\n",
    "from diff_gfdn.utils import is_unitary\n",
    "\n",
    "if config_dict.feedback_loop_config.coupling_matrix_type in (CouplingMatrixType.SCALAR, CouplingMatrixType.FILTER):\n",
    "    M_list = opt_params['feedback_loop.M']\n",
    "    num_groups = model.num_groups\n",
    "    fig, ax = plt.subplots(num_groups, 1, figsize=(6,6))\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        M = torch.from_numpy(M_list[i, ...])\n",
    "        with torch.no_grad():\n",
    "            M_ortho = model.feedback_loop.ortho_param(M)\n",
    "        ax[i].matshow(torch.abs(M_ortho))\n",
    "        ax[i].set_title(f'Room {i}')\n",
    "        is_ortho, max_val = is_unitary(M_ortho)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_path}/individual_feedback_matrices.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Observe the input gains, coupling matrix and the coupled mixing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "coupled_feedback_matrix = opt_params['coupled_feedback_matrix']\n",
    "input_gains = opt_params['input_gains'][0]\n",
    "output_gains = opt_params['output_gains'][0]\n",
    "print(f'Norm of input gains {np.linalg.norm(input_gains)}')\n",
    "print(f'Norm of output gains {np.linalg.norm(output_gains)}')\n",
    "\n",
    "if config_dict.feedback_loop_config.coupling_matrix_type == CouplingMatrixType.SCALAR:\n",
    "    # assert is_unitary(torch.from_numpy(coupled_feedback_matrix))[0]    \n",
    "    coupling_matrix = opt_params['coupling_matrix']\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    plt.matshow(np.abs(coupling_matrix), fignum=False)\n",
    "    plt.colorbar()\n",
    "    plt.title('Coupling matrix')\n",
    "    plt.subplot(212)\n",
    "    plt.matshow(np.abs(coupled_feedback_matrix), fignum=False)\n",
    "    plt.colorbar()\n",
    "    plt.title('Coupled feedback matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_path}/scalar_coupling_matrix.png')\n",
    "\n",
    "\n",
    "elif config_dict.feedback_loop_config.coupling_matrix_type == CouplingMatrixType.FILTER:\n",
    "    # assert is_paraunitary(torch.from_numpy(coupled_feedback_matrix))[0]\n",
    "    coupling_matrix = opt_params['coupling_matrix']\n",
    "    num_freq_bins = 2**10\n",
    "    plot_polynomial_matrix_magnitude_response(coupling_matrix, model.sample_rate, num_freq_bins, \n",
    "                                             'Coupling matrix response', save_path=f'{fig_path}/pu_coupling_matrix.png')\n",
    "else:\n",
    "    feedback_matrix = opt_params['coupled_feedback_matrix']\n",
    "    unit_flag, max_val = is_unitary(torch.tensor(feedback_matrix), max_tol=1e-4)\n",
    "    assert unit_flag\n",
    "    plt.figure()\n",
    "    plt.matshow(np.abs(feedback_matrix))\n",
    "    plt.title('Optimised feedback matrix')\n",
    "    plt.savefig(f'{fig_path}/random_coupling_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Plot magnitude response of each sub-FDN to inspect colouration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{fig_path}/{config_name}_mag_spectrum.png'\n",
    "plot_magnitude_response(room_data, config_dict, model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Plot NED pre and post optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = room_data.sample_rate\n",
    "mixing_time_samp = ms_to_samps(20.0, fs)\n",
    "crop_end_samp = ms_to_samps(5.0, fs)\n",
    "trunc_true_ir = h_true[mixing_time_samp:-crop_end_samp]\n",
    "len_ir = len(trunc_true_ir)\n",
    "time = np.linspace(0, (len_ir-1)/ config_dict.sample_rate, len_ir-1)\n",
    "\n",
    "ned_fdn = np.zeros((len_ir-1, max_epochs))\n",
    "ned_true = normalised_echo_density(h_true[mixing_time_samp:-crop_end_samp], \n",
    "                                   config_dict.sample_rate, window_length_ms=50)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(time, ned_true, label='Reference')\n",
    "iterate_over_epochs = [-1, max_epochs-1]\n",
    "for k, epoch in zip(range(len(iterate_over_epochs)), iterate_over_epochs):\n",
    "    h_cur = h_approx_list[k]\n",
    "    ned_fdn[:, epoch] = normalised_echo_density(h_cur[mixing_time_samp: mixing_time_samp + len(trunc_true_ir)], \n",
    "                                                config_dict.sample_rate, window_length_ms=50)\n",
    "    ax.plot(time, ned_fdn[:, epoch], label=f'GFDN, Epoch={epoch}')\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('NED')\n",
    "ax.legend()\n",
    "ax.set_xlim([0.001, max(time)])\n",
    "fig.savefig(f'{fig_path}/{config_name}_ned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Plot the amplitude distribution for each RIR as a position of space for 1kHz band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import diff_gfdn\n",
    "reload(diff_gfdn.plot)\n",
    "from diff_gfdn.plot import plot_amps_in_space, plot_edc_error_in_space\n",
    "\n",
    "plot_edc_error_in_space(room_data, all_rirs, all_pos, freq_to_plot=None,save_path=f'{fig_path}/{config_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
