{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from torch import nn\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import Optional, List\n",
    "from IPython import display\n",
    "import soundfile as sf\n",
    "from loguru import logger\n",
    "from copy import deepcopy\n",
    "\n",
    "os.chdir('..')  # This changes the working directory to DiffGFDN\n",
    "\n",
    "from diff_gfdn.dataloader import load_dataset, RIRData\n",
    "from diff_gfdn.config.config import DiffGFDNConfig, CouplingMatrixType\n",
    "from diff_gfdn.solver import convert_common_slopes_rir_to_room_dataset\n",
    "from diff_gfdn.model import DiffGFDNVarReceiverPos\n",
    "from diff_gfdn.utils import is_unitary, db2lin, db, ms_to_samps, get_response\n",
    "from diff_gfdn.plot import plot_edr, animate_coupled_feedback_matrix, plot_amps_in_space\n",
    "from diff_gfdn.analysis import get_decay_fit_net_params\n",
    "from diff_gfdn.colorless_fdn.utils import get_colorless_fdn_params\n",
    "from src.run_model import load_and_validate_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/'\n",
    "fig_path = 'figures/'\n",
    "config_name = 'synth_data_broadband_two_coupled_rooms_grid_training_colorless_loss.yml'\n",
    "config_file = config_path + config_name\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_data = convert_common_slopes_rir_to_room_dataset(config_dict.room_dataset_path, \n",
    "                                                      num_freq_bins=config_dict.trainer_config.num_freq_bins,\n",
    "                                                      )\n",
    "\n",
    "config_dict = config_dict.copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# force the trainer config device to be CPU\n",
    "if trainer_config.device != 'cpu':\n",
    "    trainer_config = trainer_config.copy(update={\"device\": 'cpu'})\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, train_valid_split_ratio=1.0,\n",
    "    batch_size=trainer_config.batch_size, shuffle=False)\n",
    "\n",
    "# get the colorless FDN params\n",
    "if config_dict.colorless_fdn_config.use_colorless_prototype:\n",
    "    colorless_fdn_params = get_colorless_fdn_params(config_dict)\n",
    "else:\n",
    "    colorless_fdn_params = None\n",
    "\n",
    "# initialise the model\n",
    "model = DiffGFDNVarReceiverPos(config_dict.sample_rate,\n",
    "                 config_dict.num_groups,\n",
    "                 config_dict.delay_length_samps,\n",
    "                 trainer_config.device,\n",
    "                 config_dict.feedback_loop_config,\n",
    "                 config_dict.output_filter_config,\n",
    "                 use_absorption_filters=False,\n",
    "                 common_decay_times=room_data.common_decay_times,\n",
    "                 colorless_fdn_params=colorless_fdn_params,\n",
    "                 use_colorless_loss=trainer_config.use_colorless_loss\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory  = Path(\"audio/\")\n",
    "fig_path = Path(\"figures\").resolve()\n",
    "checkpoint_dir = Path(trainer_config.train_dir + 'checkpoints/').resolve()\n",
    "max_epochs = trainer_config.max_epochs\n",
    "plot_ir = True \n",
    "pos_to_investigate = [1.21, 2.92, 0.83] \n",
    "desired_filename = f'ir_({pos_to_investigate[0]:.2f}, {pos_to_investigate[1]:.2f}, {pos_to_investigate[2]:.2f}).wav'\n",
    "\n",
    "# find amplitudes corresponding to the receiver position\n",
    "rec_pos_idx = np.argwhere(\n",
    "    np.all(np.round(room_data.receiver_position,2) == pos_to_investigate, axis=1))[0]\n",
    "amps_at_pos = np.squeeze(room_data.amplitudes[rec_pos_idx, :])\n",
    "h_true = np.squeeze(room_data.rirs[rec_pos_idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Iterate through epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_approx_list = []\n",
    "output_gains = []\n",
    "input_gains = []\n",
    "input_scalars = []\n",
    "output_scalars = []\n",
    "coupled_feedback_matrix = []\n",
    "coupling_matrix = []\n",
    "all_pos = []\n",
    "all_rirs = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # load the trained weights for the particular epoch\n",
    "    checkpoint = torch.load(f'{checkpoint_dir}/model_e{epoch}.pt', weights_only=True, map_location=torch.device('cpu'))\n",
    "    # Load the trained model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    break_outer_loop = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        param_dict = model.get_param_dict()\n",
    "        input_gains.append(deepcopy(param_dict['input_gains']))\n",
    "        input_scalars.append(deepcopy(param_dict['input_scalars']))\n",
    "        output_gains.append(deepcopy(param_dict['output_gains']))\n",
    "        coupled_feedback_matrix.append(deepcopy(param_dict['coupled_feedback_matrix']))\n",
    "        coupling_matrix.append(deepcopy(param_dict['coupling_matrix']))\n",
    "    \n",
    "        for data in train_dataset:\n",
    "            position = data['listener_position']\n",
    "            if trainer_config.use_colorless_loss:\n",
    "                H, H_sub_fdn, h = get_response(data, model)\n",
    "            else:\n",
    "                H, h = get_response(data, model)            \n",
    "            for num_pos in range(position.shape[0]):\n",
    "                if epoch == max_epochs - 1:\n",
    "                    all_pos.append(position[num_pos])\n",
    "                    all_rirs.append(h[num_pos, :])\n",
    "                filename = f'ir_({position[num_pos,0]:.2f}, {position[num_pos, 1]:.2f}, {position[num_pos, 2]:.2f}).wav'\n",
    "                \n",
    "                if filename == desired_filename:\n",
    "                    # get the ir at this position\n",
    "                    h_approx_list.append(h[num_pos, :])\n",
    "    \n",
    "                    # get the gains for this position\n",
    "                    if 'output_scalars' in param_dict.keys():\n",
    "                        output_scalars.append(deepcopy(param_dict['output_scalars'][num_pos]))\n",
    "\n",
    "                    # not breaking the loop to collect all the RIRs\n",
    "                    # break_outer_loop = True\n",
    "                    # break\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Plot the EDCs as a function of epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_edc(h_true: ArrayLike, h_approx: List[ArrayLike], fs: float, pos_to_investigate: List, amps_at_pos: List, mixing_time_ms:float=20.0):\n",
    "    \"\"\"Plot true and synthesised EDC curves\"\"\"\n",
    "    \n",
    "    mixing_time_samp = ms_to_samps(mixing_time_ms, fs)\n",
    "    crop_end_samp = ms_to_samps(5.0, fs)\n",
    "    \n",
    "    trunc_true_ir = h_true[mixing_time_samp:-crop_end_samp]\n",
    "    true_edf = np.flipud(np.cumsum(np.flipud(trunc_true_ir**2), axis=-1))\n",
    "    time = np.linspace(0, (len(trunc_true_ir) - 1) / fs,\n",
    "                       len(trunc_true_ir))\n",
    "  \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(time, db(true_edf, is_squared=True), label='True EDF')\n",
    "    ax.plot(np.zeros(len(amps_at_pos)), db(amps_at_pos, is_squared=True), 'kx')\n",
    "\n",
    "    num_epochs = len(h_approx)\n",
    "    for epoch in range(num_epochs):\n",
    "        approx_ir = h_approx[epoch]\n",
    "        trunc_approx_ir = approx_ir[mixing_time_samp: mixing_time_samp + len(trunc_true_ir)]\n",
    "        synth_edf = np.flipud(np.cumsum(np.flipud(trunc_approx_ir**2), axis=-1))\n",
    "        ax.plot(time, db(synth_edf, is_squared=True), label=f'Epoch={epoch}')\n",
    "        ax.legend()\n",
    "        \n",
    "        display.display(fig)  # Display the updated figure\n",
    "        display.clear_output(wait=True)  # Clear the previous output to keep updates in place\n",
    "        plt.pause(0.1)\n",
    "        \n",
    "    ax.set_title(\n",
    "        f'Truncated EDF at position {pos_to_investigate[0]:.2f}, {pos_to_investigate[1]:.2f}, {pos_to_investigate[2]:.2f} m'\n",
    "    )\n",
    "    fig.savefig(Path(f'{fig_path}/compare_synth_edf_{pos_to_investigate}_{config_name}.png').resolve())\n",
    "    plt.show()\n",
    "\n",
    "plot_edc(h_true, h_approx_list, config_dict.sample_rate, pos_to_investigate, amps_at_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Plot the learnt feedback matrix and input gains for the grid, plot the output gains for a single position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.plot import animate_coupled_feedback_matrix\n",
    "\n",
    "if config_dict.feedback_loop_config.coupling_matrix_type == CouplingMatrixType.SCALAR:\n",
    "    animate_coupled_feedback_matrix(np.abs(coupled_feedback_matrix), np.abs(coupling_matrix), \n",
    "                                    save_path=Path(f'{fig_path}/animation/{config_name}_{pos_to_investigate}_scalar_coupling_matrix.gif').resolve())\n",
    "else:\n",
    "    animate_coupled_feedback_matrix(np.abs(coupled_feedback_matrix), \n",
    "                                    save_path=Path(f'{fig_path}/animation/{config_name}_{pos_to_investigate}_random_coupling_matrix.gif').resolve())\n",
    "\n",
    "# also save the final optimised matrix\n",
    "if config_dict.feedback_loop_config.coupling_matrix_type == CouplingMatrixType.SCALAR:\n",
    "    assert is_unitary(torch.from_numpy(coupled_feedback_matrix[-1]))[0]    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(4, 6))\n",
    "\n",
    "    # Coupling matrix subplot\n",
    "    cax1 = ax1.matshow(np.abs(coupling_matrix[-1]))\n",
    "    fig.colorbar(cax1, ax=ax1)\n",
    "    ax1.set_title('Coupling matrix')\n",
    "    \n",
    "    # Coupled feedback matrix subplot\n",
    "    cax2 = ax2.matshow(np.abs(coupled_feedback_matrix[-1]))\n",
    "    fig.colorbar(cax2, ax=ax2)\n",
    "    ax2.set_title('Coupled feedback matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{fig_path}/{config_name}_scalar_coupling_matrix.png', bbox_inches='tight')\n",
    "else:\n",
    "    unit_flag, max_val = is_unitary(torch.tensor(coupled_feedback_matrix[-1]), max_tol=1e-4)\n",
    "    assert unit_flag\n",
    "    plt.figure()\n",
    "    plt.matshow(np.abs(coupled_feedback_matrix[-1]))\n",
    "    plt.title('Optimised feedback matrix')\n",
    "    plt.savefig(f'{fig_path}/{config_name}_random_coupling_matrix.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of vectors to a 2D numpy array (N x M matrix)\n",
    "input_gain_matrix = np.stack([vec for vec in input_scalars]).T\n",
    "output_gain_matrix = np.stack([vec for vec in output_scalars]).T\n",
    "\n",
    "# Plot the matrix\n",
    "fig, ax = plt.subplots(2, figsize=(6,6))\n",
    "in_plot = ax[0].matshow(np.abs(input_gain_matrix), aspect='auto', cmap='viridis')\n",
    "fig.colorbar(in_plot, label=\"dB\", ax=ax[0])\n",
    "ax[0].set_ylabel(\"Group number\")\n",
    "ax[0].set_xlabel(\"Epoch number\")\n",
    "ax[0].set_title(\"Input scalars vs epoch\")\n",
    "\n",
    "out_plot = ax[1].matshow(np.abs(output_gain_matrix), aspect='auto', cmap='viridis')\n",
    "fig.colorbar(out_plot, label=\"dB\", ax=ax[1])\n",
    "ax[1].set_ylabel(\"Group number\")\n",
    "ax[1].set_xlabel(\"Epoch number\")\n",
    "ax[1].set_title(\"Output scalars vs epoch\")\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "fig.savefig(Path(f'{fig_path}/{config_name}_{pos_to_investigate}_io_scalars.png').resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Plot the amplitude distribution for each RIR as a position of space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import diff_gfdn\n",
    "reload(diff_gfdn.plot)\n",
    "from diff_gfdn.plot import plot_amps_in_space\n",
    "\n",
    "plot_amps_in_space(room_data, all_rirs, all_pos, freq_to_plot=None, scatter=True, save_path=f'{fig_path}/{config_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
