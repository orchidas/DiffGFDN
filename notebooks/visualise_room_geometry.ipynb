{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Optional\n",
    "from IPython import display\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "os.chdir('..')  # This changes the working directory to DiffGFDN\n",
    "\n",
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset, custom_collate, RoomDataset\n",
    "from diff_gfdn.gain_filters import OneHotEncoding, SinusoidalEncoding\n",
    "from src.run_model import load_and_validate_config\n",
    "from diff_gfdn.solver import convert_common_slopes_rir_to_room_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Visualise the 3D mesh of the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/'\n",
    "fig_path = 'figures/'\n",
    "config_name = 'treble_data_grid_training_full_band_colorless_loss.yml'\n",
    "config_file = config_path + config_name\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"3room_FDTD\" in config_dict.room_dataset_path:\n",
    "    room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve(), config_dict)\n",
    "else:\n",
    "    room_data = convert_common_slopes_rir_to_room_dataset(config_dict.room_dataset_path, \n",
    "                                                          num_freq_bins=config_dict.trainer_config.num_freq_bins,\n",
    "                                                          )\n",
    "\n",
    "config_dict = config_dict.model_copy(update={\"num_groups\": room_data.num_rooms})\n",
    "room_data.plot_2D_meshgrid(room_data.mesh_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Check if the one-hot encoding works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_receiver_points(room_data: RoomDataset, rec_pos: torch.Tensor, col:str = 'k', title: Optional[str]=None):\n",
    "    \"\"\"Plot the 3D receiver points to see if one-hot encoding works\"\"\"\n",
    "    if torch.is_tensor(rec_pos):\n",
    "        rec_pos = rec_pos.cpu().detach().numpy()\n",
    "    x_rec = rec_pos[:, 0]\n",
    "    y_rec = rec_pos[:, 1]\n",
    "\n",
    "    # Plot using scatter without any additional data for color\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Plot the X, Y, Z points\n",
    "    ax.scatter(x_rec, y_rec, color=col, marker='x')\n",
    "\n",
    "    # Set the limits for all axes\n",
    "    ax.set_xlim(0,\n",
    "                room_data.room_dims[-1][0] + room_data.room_start_coord[-1][0] + 0.5)\n",
    "    ax.set_ylim(0,\n",
    "                room_data.room_dims[-1][1] + room_data.room_start_coord[-1][1] + 0.5)\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    if title is None:\n",
    "        ax.set_title('Receiver grid')\n",
    "    else:\n",
    "        ax.set_title(f'Receiver grid, {title}')\n",
    "    # Set a top-down view (90-degree elevation)\n",
    "    # ax.view_init(elev=90, azim=-90)  # Adjust azimuth for better alignment if needed\n",
    "\n",
    "    # Show the plot\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of groups to the config dictionary\n",
    "config_dict = config_dict.model_copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "if config_dict.sample_rate != room_data.sample_rate:\n",
    "    logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "    config_dict.sample_rate = sample_rate\n",
    "\n",
    "# get the training config.\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, 0.8,\n",
    "    trainer_config.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_points = []\n",
    "for data in train_dataset:\n",
    "    encoder = OneHotEncoding()\n",
    "    one_hot_encode, closest_point, _ = encoder(data['mesh_2D'], data['listener_position'])\n",
    "    closest_points.append(closest_point)\n",
    "\n",
    "closest_points = torch.stack(closest_points)\n",
    "closest_points = closest_points.view(-1, closest_points.shape[-1])\n",
    "\n",
    "fig, ax = plot_2D_receiver_points(room_data, closest_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "####Â Validation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_points_valid = []\n",
    "for data in valid_dataset:\n",
    "    encoder = OneHotEncoding()\n",
    "    one_hot_encode, closest_point, _ = encoder(data['mesh_2D'], data['listener_position'])\n",
    "    closest_points_valid.append(closest_point)\n",
    "\n",
    "closest_points_valid = torch.stack(closest_points_valid)\n",
    "closest_points_valid = closest_points_valid.view(-1, closest_points_valid.shape[-1])\n",
    "fig, ax = plot_2D_receiver_points(room_data, closest_points_valid, col='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Plot the Fourier encodings to see if it can capture spatial variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slope2noise.rooms import RoomGeometry\n",
    "room = RoomGeometry(room_data.sample_rate,\n",
    "                    room_data.num_rooms,\n",
    "                    np.array(room_data.room_dims),\n",
    "                    np.array(room_data.room_start_coord),\n",
    "                    aperture_coords=room_data.aperture_coords)\n",
    "\n",
    "npos = 0\n",
    "num_encoded_features = 3 * 2 * config_dict.output_filter_config.num_fourier_features\n",
    "encoded_positions = np.zeros((room_data.num_rec, num_encoded_features))\n",
    "shuffled_positions = np.zeros((room_data.num_rec, 3))\n",
    "\n",
    "for data in train_dataset:\n",
    "    encoder = SinusoidalEncoding(num_fourier_features=config_dict.output_filter_config.num_fourier_features)\n",
    "    encoded_positions[npos*trainer_config.batch_size:(npos+1)*trainer_config.batch_size, :] = encoder(data['norm_listener_position'])\n",
    "    shuffled_positions[npos*trainer_config.batch_size:(npos+1)*trainer_config.batch_size, :] = data['listener_position']\n",
    "    npos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(config_dict.output_filter_config.num_fourier_features):\n",
    "    room.plot_amps_at_receiver_points(\n",
    "            shuffled_positions,\n",
    "            np.squeeze(np.array(room_data.source_position)),\n",
    "            encoded_positions[:, k*6:k*6 + 3].T,\n",
    "            scatter_plot=False,\n",
    "            title=f'Sine Fourier encodings, order = {k}',\n",
    "            save_path=f'figures/spatial_encodings/sine_encoding_order={k}.png')\n",
    "\n",
    "    room.plot_amps_at_receiver_points(\n",
    "                shuffled_positions,\n",
    "                np.squeeze(np.array(room_data.source_position)),\n",
    "                encoded_positions[:, k*6 + 3:k*6 + 6].T,\n",
    "                scatter_plot=False,\n",
    "                title=f'Cosine Fourier encodings, order = {k}',\n",
    "                save_path=f'figures/spatial_encodings/cosine_encoding_order={k}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### For training the CNN, \n",
    "we need to discretise the grid of receiver locations into square patches, plot the positions in each patch to investigate how uniformly they are distributed - the patches should not overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.dataloader import InputFeatures, to_device\n",
    "from spatial_sampling.config import SpatialSamplingConfig\n",
    "from spatial_sampling.dataloader import (SpatialRoomDataset, parse_room_data, load_dataset, SpatialSamplingDataset, split_dataset_by_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/spatial_sampling/'\n",
    "fig_path = 'figures/'\n",
    "config_name = 'treble_data_grid_training_1000Hz_directional_spatial_sampling_test_cnn.yml'\n",
    "config_file = config_path + config_name\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       SpatialSamplingConfig)\n",
    "config_dict = config_dict.model_copy(update={\"batch_size\": 16})\n",
    "\n",
    "\n",
    "room_data = parse_room_data(\n",
    "            Path(config_dict.room_dataset_path).resolve())\n",
    "grid_resolution_m = np.arange(config_dict.num_grid_spacing, 0,\n",
    "                              -1) * room_data.grid_spacing_m\n",
    "\n",
    "# create the dataset\n",
    "dataset = SpatialSamplingDataset(\n",
    "    config_dict.device,\n",
    "    room_data,\n",
    ")\n",
    "dataset = to_device(dataset, config_dict.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatial_sampling\n",
    "reload(spatial_sampling.dataloader)\n",
    "from spatial_sampling.dataloader import load_dataset\n",
    "\n",
    "# loop over different grid resolutions\n",
    "for k in range(len(grid_resolution_m)):\n",
    "\n",
    "    # split data into training and validation set\n",
    "    train_set, valid_set = split_dataset_by_resolution(dataset,\n",
    "                                                       grid_resolution_m[k])\n",
    "    \n",
    "    train_dataset, valid_dataset, dataset_ref = load_dataset(\n",
    "        room_data,\n",
    "        config_dict.device,\n",
    "        grid_resolution_m=np.round(grid_resolution_m[k], 1),\n",
    "        network_type=config_dict.network_type,\n",
    "        batch_size=config_dict.batch_size)\n",
    "    \n",
    "    # plot al points in training subset\n",
    "    subset_listener_pos_idx = train_set.indices\n",
    "    train_subset_listener_pos = dataset_ref.listener_positions[subset_listener_pos_idx, :2]\n",
    "    fig, ax = plot_2D_receiver_points(room_data, train_subset_listener_pos, col='r', title=f'Grid spacing = {np.round(grid_resolution_m[k], 1)}m')\n",
    "    \n",
    "    # loop through points in each batch\n",
    "    for batch in train_dataset:\n",
    "        cur_rec_pos = batch['listener_position']\n",
    "        ax.scatter(cur_rec_pos[:, 0], cur_rec_pos[:, 1], color='k', marker='o')\n",
    "        display.display(fig)  # Display the updated figure\n",
    "        display.clear_output(wait=True)  # Clear the previous output to keep updates in place\n",
    "        plt.pause(1.0)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
