{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset, custom_collate, RoomDataset\n",
    "from diff_gfdn.gain_filters import OneHotEncoding\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Visualise the 3D mesh of the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DiffGFDNConfig()\n",
    "room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve())\n",
    "room_data.plot_3D_meshgrid(room_data.mesh_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Check if the one-hot encoding works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_receiver_points(room_data: RoomDataset, rec_points: torch.Tensor):\n",
    "    \"\"\"Plot the 3D receiver points to see if one-hot encoding works\"\"\"\n",
    "    rec_pos = rec_points.cpu().detach().numpy()\n",
    "    x_rec = rec_pos[:, 0]\n",
    "    y_rec = rec_pos[:, 1]\n",
    "    z_rec = rec_pos[:, 2]\n",
    "\n",
    "    # Plot using scatter without any additional data for color\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the X, Y, Z points\n",
    "    ax.scatter(x_rec, y_rec, z_rec, color='k', marker='x')\n",
    "\n",
    "    # Set the limits for all axes\n",
    "    ax.set_xlim(0,\n",
    "                room_data.room_dims[-1][0] + room_data.room_start_coord[-1][0] + 0.5)\n",
    "    ax.set_ylim(0,\n",
    "                room_data.room_dims[-1][1] + room_data.room_start_coord[-1][1] + 0.5)\n",
    "    ax.set_zlim(0, room_data.room_dims[-1][-1] + 0.5)\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.set_title('Receiver grid')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of groups to the config dictionary\n",
    "config_dict = config_dict.copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "if config_dict.sample_rate != room_data.sample_rate:\n",
    "    logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "    config_dict.sample_rate = sample_rate\n",
    "\n",
    "# get the training config\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, trainer_config.train_valid_split,\n",
    "    trainer_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_points = []\n",
    "for data in train_dataset:\n",
    "    encoder = OneHotEncoding()\n",
    "    one_hot_encode, closest_point = encoder(data['mesh_3D'], data['listener_position'])\n",
    "    closest_points.append(closest_point)\n",
    "\n",
    "closest_points = torch.stack(closest_points)\n",
    "closest_points = closest_points.view(-1, closest_points.shape[-1])\n",
    "\n",
    "plot_3D_receiver_points(room_data, closest_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Check output data shape and audio saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # initialise the model\n",
    "from diff_gfdn.model import DiffGFDN\n",
    "from diff_gfdn.utils import get_response\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torchaudio\n",
    "\n",
    "model = DiffGFDN(room_data.sample_rate, room_data.num_rooms,\n",
    "                 config_dict.delay_length_samps,\n",
    "                 room_data.absorption_coeffs, room_data.room_dims,\n",
    "                 trainer_config.device, config_dict.feedback_loop_config,\n",
    "                 config_dict.output_filter_config)\n",
    "directory  = \"../audio/\"\n",
    "\n",
    "for data in train_dataset:\n",
    "    position = data['listener_position']\n",
    "    H, h = get_response(data, model)\n",
    "    h = torch.div(h, torch.max(torch.abs(h)))\n",
    "    print(position.shape)\n",
    "    print(H.shape, h.shape)\n",
    "    h_stereo = torch.stack((h[0, :], h[0, :]), dim=1)\n",
    "    print(h_stereo.shape)\n",
    "    filename = f'ir_({position[0,0]:.2f},{position[0, 1]:.2f},{position[0, 2]:.2f}).wav'\n",
    "    filepath = os.path.join(Path(directory).resolve(), filename)\n",
    "    torchaudio.save(filepath,\n",
    "                    h_stereo,\n",
    "                    int(model.sample_rate),\n",
    "                    bits_per_sample=32,\n",
    "                    channels_first=False)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
