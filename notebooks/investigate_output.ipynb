{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Load dataset and save IRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset\n",
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.model import DiffGFDN\n",
    "from diff_gfdn.utils import get_response, db\n",
    "from diff_gfdn.losses import get_stft_torch, get_edr_from_stft\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "from numpy.typing import ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(S: torch.tensor, freqs: ArrayLike, time_frames: ArrayLike, title:Optional[str]=None):\n",
    "    plt.figure()\n",
    "    plt.imshow(db(np.abs(S)).cpu().detach().numpy(), aspect='auto', origin='lower',\n",
    "    extent=[time_frames.min(), time_frames.max(), freqs.min(), freqs.max()])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('dB')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_edr(h: torch.tensor, \n",
    "             fs: float, \n",
    "             win_size:int=2**9, \n",
    "             hop_size:int=2**8, \n",
    "             title:Optional[str]=None) -> Tuple[torch.tensor, ArrayLike, ArrayLike]:\n",
    "    S, freqs, time_frames = get_stft_torch(h, fs, win_size=win_size, hop_size=hop_size, nfft=win_size, freq_axis=0)\n",
    "    edr = get_edr_from_stft(S)\n",
    "    plot_spectrogram(edr, freqs, time_frames,title)\n",
    "    return edr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Read config files and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DiffGFDNConfig()\n",
    "room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve(), save_irs=True)\n",
    "\n",
    "# add number of groups to the config dictionary\n",
    "config_dict = config_dict.copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "if config_dict.sample_rate != room_data.sample_rate:\n",
    "    logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "    config_dict.sample_rate = sample_rate\n",
    "\n",
    "# get the training config\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, trainer_config.train_valid_split,\n",
    "    trainer_config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Check output data and compare with true IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the model\n",
    "model = DiffGFDN(room_data.sample_rate, room_data.num_rooms,\n",
    "                 config_dict.delay_length_samps,\n",
    "                 room_data.absorption_coeffs, room_data.room_dims,\n",
    "                 trainer_config.device, config_dict.feedback_loop_config,\n",
    "                 config_dict.output_filter_config)\n",
    "\n",
    "print(model)\n",
    "\n",
    "audio_directory  = Path(\"../audio/\")\n",
    "checkpoint_dir = Path(\"../output/mps/checkpoints\")\n",
    "max_epochs = 3\n",
    "save_ir_dir = audio_directory/'mps'\n",
    "save_ir = False\n",
    "plot_ir = not save_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    # load the trained weights for the particular epoch\n",
    "    checkpoint = torch.load(Path(checkpoint_dir/f'model_e{epoch}.pt').resolve(), weights_only=True)                        \n",
    "    # Load the trained model state\n",
    "    opt_params = model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    \n",
    "    for data in train_dataset:\n",
    "        position = data['listener_position']\n",
    "        H, h = get_response(data, model)\n",
    "        \n",
    "        for num_pos in range(position.shape[0]):\n",
    "            filename = f'ir_({position[num_pos,0]:.2f}, {position[num_pos, 1]:.2f}, {position[num_pos, 2]:.2f}).wav'\n",
    "            \n",
    "            # find the true IR corresponding to this position\n",
    "            filepath_true = os.path.join(Path(audio_directory/'true').resolve(), filename)\n",
    "            h_true = torch.from_numpy(sf.read(filepath_true)[0])\n",
    "\n",
    "            if plot_ir:\n",
    "                # plot the EDRs of the true and estimated\n",
    "                plot_edr(h_true, model.sample_rate, title=f'True RIR EDR, epoch={epoch}')\n",
    "                plot_edr(h[num_pos, :], model.sample_rate, title=f'Estimated RIR EDR, epoch={epoch}')\n",
    "        \n",
    "                plt.figure()\n",
    "                plt.plot(torch.stack((h_true, h[num_pos, :len(h_true)]), dim=-1))\n",
    "                plt.show()\n",
    "\n",
    "            if save_ir and (epoch == max_epochs - 1):\n",
    "                outer_loop_break = False\n",
    "                filepath = os.path.join(save_ir_dir, filename)\n",
    "                torchaudio.save(filepath,\n",
    "                            torch.stack((h[num_pos, :], h[num_pos, :]),\n",
    "                                        dim=1).cpu(),\n",
    "                            int(model.sample_rate),\n",
    "                            bits_per_sample=32,\n",
    "                            channels_first=False)\n",
    "            else:\n",
    "                outer_loop_break = True\n",
    "                break\n",
    "                \n",
    "        if outer_loop_break:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- There is no high frequency content in the estimated signal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
