{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Load dataset and save IRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.dataloader import ThreeRoomDataset, load_dataset\n",
    "from diff_gfdn.config.config import DiffGFDNConfig\n",
    "from diff_gfdn.model import DiffGFDN\n",
    "from diff_gfdn.utils import get_response, db, is_unitary, absorption_to_gain_per_sample\n",
    "from diff_gfdn.losses import get_stft_torch, get_edr_from_stft\n",
    "from run_model import load_and_validate_config\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "from numpy.typing import ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(S: torch.tensor, freqs: ArrayLike, time_frames: ArrayLike, title:Optional[str]=None):\n",
    "    plt.figure()\n",
    "    plt.imshow(db(np.abs(S)).cpu().detach().numpy(), aspect='auto', origin='lower',\n",
    "    extent=[time_frames.min(), time_frames.max(), freqs.min(), freqs.max()])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('dB')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_edr(h: torch.tensor, \n",
    "             fs: float, \n",
    "             win_size:int=2**9, \n",
    "             hop_size:int=2**8, \n",
    "             title:Optional[str]=None) -> Tuple[torch.tensor, ArrayLike, ArrayLike]:\n",
    "    S, freqs, time_frames = get_stft_torch(h, fs, win_size=win_size, hop_size=hop_size, nfft=win_size, freq_axis=0)\n",
    "    edr = get_edr_from_stft(S)\n",
    "    plot_spectrogram(edr, freqs, time_frames,title)\n",
    "    return edr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Read config files and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../data/config/'\n",
    "config_file = config_path + 'antialiasing_sample_outside_unit_circle.yml'\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)\n",
    "room_data = ThreeRoomDataset(Path(config_dict.room_dataset_path).resolve(), save_irs=False)\n",
    "\n",
    "# add number of groups to the config dictionary\n",
    "config_dict = config_dict.copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "if config_dict.sample_rate != room_data.sample_rate:\n",
    "    logger.warn(\"Config sample rate does not match data, alterning it\")\n",
    "    config_dict.sample_rate = sample_rate\n",
    "\n",
    "# get the training config\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, trainer_config.train_valid_split,\n",
    "    trainer_config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Check output data and compare with true IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the model\n",
    "model = DiffGFDN(room_data.sample_rate, room_data.num_rooms,\n",
    "                 config_dict.delay_length_samps,\n",
    "                 room_data.room_dims,\n",
    "                 trainer_config.device, \n",
    "                 config_dict.feedback_loop_config,\n",
    "                 config_dict.output_filter_config,\n",
    "                 config_dict.use_absorption_filters,\n",
    "                 common_decay_times=room_data.common_decay_times,\n",
    "                 band_centre_hz=room_data.band_centre_hz\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory  = Path(\"../audio/\")\n",
    "checkpoint_dir = Path(trainer_config.train_dir + 'checkpoints/').resolve()\n",
    "max_epochs = trainer_config.max_epochs\n",
    "save_ir_dir = Path(trainer_config.ir_dir).resolve() \n",
    "save_ir = False\n",
    "plot_ir = not save_ir  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    # load the trained weights for the particular epoch\n",
    "    checkpoint = torch.load(f'{checkpoint_dir}/model_e{epoch}.pt', weights_only=True)\n",
    "    # Load the trained model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    \n",
    "    for data in train_dataset:\n",
    "        position = data['listener_position']\n",
    "        H, h = get_response(data, model)\n",
    "        \n",
    "        for num_pos in range(position.shape[0]):\n",
    "            filename = f'ir_({position[num_pos,0]:.2f}, {position[num_pos, 1]:.2f}, {position[num_pos, 2]:.2f}).wav'\n",
    "            \n",
    "            # find the true IR corresponding to this position\n",
    "            filepath_true = os.path.join(Path(audio_directory/'true').resolve(), filename)\n",
    "            h_true = torch.from_numpy(sf.read(filepath_true)[0])\n",
    "\n",
    "            if plot_ir:\n",
    "                # plot the EDRs of the true and estimated\n",
    "                plot_edr(h_true, model.sample_rate, title=f'True RIR EDR, epoch={epoch}')\n",
    "                plot_edr(h[num_pos, :], model.sample_rate, title=f'Estimated RIR EDR, epoch={epoch}')\n",
    "        \n",
    "                plt.figure()\n",
    "                plt.plot(torch.stack((h_true, h[num_pos, :len(h_true)]), dim=-1))\n",
    "                plt.show()\n",
    "\n",
    "            if save_ir and (epoch == max_epochs - 1):\n",
    "                outer_loop_break = False\n",
    "                filepath = os.path.join(save_ir_dir, filename)\n",
    "                torchaudio.save(filepath,\n",
    "                            torch.stack((h[num_pos, :], h[num_pos, :]),\n",
    "                                        dim=1).cpu(),\n",
    "                            int(model.sample_rate),\n",
    "                            bits_per_sample=32,\n",
    "                            channels_first=False)\n",
    "            else:\n",
    "                outer_loop_break = True\n",
    "                break\n",
    "                \n",
    "        if outer_loop_break:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Get the final trained parameters and investigate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "param_path = Path('../output/mps/parameters_opt.mat')\n",
    "opt_params = loadmat(param_path.resolve())\n",
    "print(opt_params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Observe the individual mixing matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import diff_gfdn\n",
    "reload(diff_gfdn.utils)\n",
    "from diff_gfdn.utils import is_unitary\n",
    "\n",
    "M_list = opt_params['feedback_loop.M']\n",
    "num_groups = model.num_groups\n",
    "fig, ax = plt.subplots(num_groups, 1, figsize=(6,6))\n",
    "\n",
    "for i in range(num_groups):\n",
    "    M = torch.from_numpy(M_list[i, ...])\n",
    "    with torch.no_grad():\n",
    "        M_ortho = model.feedback_loop.ortho_param(M)\n",
    "    ax[i].matshow(torch.abs(M_ortho))\n",
    "    ax[i].set_title(f'Room {i}')\n",
    "    is_ortho, max_val = is_unitary(M_ortho)\n",
    "    print(max_val)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Observe the input gains, coupling matrix and the coupled mixing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = opt_params['feedback_loop.alpha'][0]\n",
    "coupled_feedback_matrix = opt_params['coupled_feedback_matrix']\n",
    "assert is_unitary(torch.from_numpy(coupled_feedback_matrix))[0]\n",
    "\n",
    "input_gains = opt_params['input_gains'][0]\n",
    "print(np.linalg.norm(input_gains))\n",
    "\n",
    "with torch.no_grad():\n",
    "    coupling_matrix =model.feedback_loop.nd_unitary(torch.from_numpy(alpha), room_data.num_rooms)\n",
    "    unit_flag, max_val = is_unitary(coupling_matrix)\n",
    "    assert unit_flag\n",
    "    \n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.matshow(torch.abs(coupling_matrix), fignum=False)\n",
    "plt.colorbar()\n",
    "plt.title('Coupling matrix')\n",
    "plt.subplot(212)\n",
    "plt.matshow(np.abs(coupled_feedback_matrix), fignum=False)\n",
    "plt.colorbar()\n",
    "plt.title('Coupled feedback matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Plot the output filter response for a few positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import sosfreqz\n",
    "output_biquad_coeffs = opt_params['output_biquad_coeffs']\n",
    "delays = opt_params['delays'][0]\n",
    "\n",
    "for b in range(len(output_biquad_coeffs)):\n",
    "    fig, ax = plt.subplots()  # Create a new figure and axis for each outer loop iteration\n",
    "    for n in range(len(delays)):\n",
    "        cur_biquad_coeffs = output_biquad_coeffs[b][n]\n",
    "        # ensure a0 = 1 (needed by scipy)\n",
    "        for k in range(cur_biquad_coeffs.shape[0]):\n",
    "            cur_biquad_coeffs[k,:] /= cur_biquad_coeffs[k, 3]\n",
    "\n",
    "        freqs, filt_response = sosfreqz(cur_biquad_coeffs, worN=2**9, fs=room_data.sample_rate)\n",
    "        ax.semilogx(freqs, 20*np.log10(np.abs(filt_response)), label=f'Line {n}')\n",
    "        \n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Magnitude (dB)')\n",
    "    ax.set_title(f'Output filter for position {b}')\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
