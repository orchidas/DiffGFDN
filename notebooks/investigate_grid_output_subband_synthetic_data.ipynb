{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from torch import nn\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import Optional, List\n",
    "from IPython import display\n",
    "import soundfile as sf\n",
    "from loguru import logger\n",
    "from copy import deepcopy\n",
    "\n",
    "from diff_gfdn.dataloader import load_dataset, RIRData\n",
    "from diff_gfdn.config.config import DiffGFDNConfig, CouplingMatrixType\n",
    "from diff_gfdn.solver import convert_common_slopes_rir_to_room_dataset\n",
    "from diff_gfdn.model import DiffGFDNVarReceiverPos\n",
    "from diff_gfdn.utils import is_unitary, db2lin, db, ms_to_samps, get_response\n",
    "from diff_gfdn.plot import plot_edr, animate_coupled_feedback_matrix, plot_subband_edc, plot_learned_svf_response, plot_amps_in_space\n",
    "from diff_gfdn.analysis import get_decay_fit_net_params\n",
    "from run_model import load_and_validate_config\n",
    "os.chdir('..')  # This changes the working directory to DiffGFDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'data/config/'\n",
    "fig_path = 'figures/'\n",
    "config_name = 'synth_data_subband_two_coupled_rooms_grid_training.yml'\n",
    "config_file = config_path + config_name\n",
    "config_dict = load_and_validate_config(config_file,\n",
    "                                       DiffGFDNConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_data = convert_common_slopes_rir_to_room_dataset(config_dict.room_dataset_path, \n",
    "                                                      num_freq_bins=config_dict.trainer_config.num_freq_bins,\n",
    "                                                      )\n",
    "\n",
    "config_dict = config_dict.copy(update={\"num_groups\": room_data.num_rooms})\n",
    "\n",
    "trainer_config = config_dict.trainer_config\n",
    "\n",
    "# force the trainer config device to be CPU\n",
    "if trainer_config.device != 'cpu':\n",
    "    trainer_config = trainer_config.copy(update={\"device\": 'cpu'})\n",
    "\n",
    "# prepare the training and validation data for DiffGFDN\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    room_data, trainer_config.device, train_valid_split_ratio=1.0,\n",
    "    batch_size=trainer_config.batch_size, shuffle=False)\n",
    "\n",
    "# initialise the model\n",
    "model = DiffGFDNVarReceiverPos(config_dict.sample_rate,\n",
    "                 config_dict.num_groups,\n",
    "                 config_dict.delay_length_samps,\n",
    "                 trainer_config.device,\n",
    "                 config_dict.feedback_loop_config,\n",
    "                 config_dict.output_filter_config,\n",
    "                 use_absorption_filters=True,\n",
    "                 common_decay_times=room_data.common_decay_times,\n",
    "                 band_centre_hz=room_data.band_centre_hz,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory  = Path(\"audio/\")\n",
    "fig_path = Path(\"figures\").resolve()\n",
    "checkpoint_dir = Path(trainer_config.train_dir + 'checkpoints/').resolve()\n",
    "max_epochs = trainer_config.max_epochs\n",
    "plot_ir = True \n",
    "pos_to_investigate = [2.41, 5.54, 1.10] #[0.56, 4.27, 0.65]\n",
    "desired_filename = f'ir_({pos_to_investigate[0]:.2f}, {pos_to_investigate[1]:.2f}, {pos_to_investigate[2]:.2f}).wav'\n",
    "\n",
    "# find amplitudes corresponding to the receiver position\n",
    "rec_pos_idx = np.where(\n",
    "    np.all(np.round(room_data.receiver_position, 2) == pos_to_investigate, axis=1))[0]\n",
    "amplitudes = room_data.amplitudes[..., rec_pos_idx].T\n",
    "h_true = np.squeeze(room_data.rirs[rec_pos_idx, :])\n",
    "if room_data.num_rooms == 1:\n",
    "    amplitudes = amplitudes[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Iterate through epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_approx_list = []\n",
    "output_gains = []\n",
    "input_gains = []\n",
    "coupled_feedback_matrix = []\n",
    "coupling_matrix = []\n",
    "output_biquad_coeffs = []\n",
    "svf_params = []\n",
    "all_pos = []\n",
    "all_rirs = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # load the trained weights for the particular epoch\n",
    "    checkpoint = torch.load(f'{checkpoint_dir}/model_e{epoch}.pt', weights_only=True, map_location=torch.device('cpu'))\n",
    "    # Load the trained model state\n",
    "    model.load_state_dict(checkpoint)\n",
    "    # in eval mode, no gradients are calculated\n",
    "    model.eval()\n",
    "    break_outer_loop = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        param_dict = model.get_param_dict()\n",
    "        input_gains.append(deepcopy(param_dict['input_gains']))\n",
    "\n",
    "        if 'output_gains' in param_dict.keys():\n",
    "            output_gains.append(deepcopy(param_dict['output_gains']))\n",
    "        if 'coupled_feedback_matrix' in param_dict.keys():\n",
    "            coupled_feedback_matrix.append(deepcopy(param_dict['coupled_feedback_matrix']))\n",
    "        if 'coupling_matrix' in param_dict.keys():\n",
    "            coupling_matrix.append(deepcopy(param_dict['coupling_matrix']))\n",
    "        if 'random_feedback_matrix' in param_dict.keys():\n",
    "            coupled_feedback_matrix.append(deepcopy(param_dict['random_feedback_matrix']))\n",
    "       \n",
    "                            \n",
    "        for data in train_dataset:\n",
    "            position = data['listener_position']\n",
    "            H, h = get_response(data, model)\n",
    "            \n",
    "            for num_pos in range(position.shape[0]):\n",
    "                if epoch == max_epochs - 1:\n",
    "                    all_pos.append(position[num_pos])\n",
    "                    all_rirs.append(h[num_pos, :])\n",
    "                filename = f'ir_({position[num_pos,0]:.2f}, {position[num_pos, 1]:.2f}, {position[num_pos, 2]:.2f}).wav'\n",
    "                \n",
    "                if filename == desired_filename:\n",
    "                    # get the ir at this position\n",
    "                    h_approx_list.append(h[num_pos, :])\n",
    "    \n",
    "                    # get the gains for this position\n",
    "                    try:\n",
    "                        output_biquad_coeffs.append(deepcopy(param_dict['output_biquad_coeffs'][num_pos]))\n",
    "                        svf_params.append(deepcopy(param_dict['output_svf_params'][num_pos]))\n",
    "                    except Exception as e:\n",
    "                        logger.warning(e)\n",
    "                        continue\n",
    "    \n",
    "                    # plot the EDRs of the true and estimated\n",
    "                    plot_edr(h_true, model.sample_rate, title=f'True RIR EDR, epoch={epoch}', \n",
    "                             save_path=f'{fig_path}/true_edr_{filename}_{config_name}_epoch={epoch}.png')\n",
    "    \n",
    "                    plot_edr(h[num_pos, :], model.sample_rate, title=f'Estimated RIR EDR, epoch={epoch}', \n",
    "                             save_path=f'{fig_path}/approx_edr_{filename}_{config_name}_epoch={epoch}.png')\n",
    "            \n",
    "                    plt.plot(torch.stack((h_true, h[num_pos, :len(h_true)]), dim=-1))\n",
    "                    plt.xlim([0, int(1.5 * model.sample_rate)])\n",
    "                    plt.savefig(f'{fig_path}/ir_compare_{filename}_{config_name}_epoch={epoch}.png')\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Plot subband EDC as function of epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_subband_edc(h_true, h_approx_list, config_dict.sample_rate, room_data.band_centre_hz, pos_to_investigate, \n",
    "                 save_path=f'{fig_path}/compare_synth_edf_{pos_to_investigate}_{config_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Investigate output SVFs as a function of epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import diff_gfdn\n",
    "reload(diff_gfdn.plot)\n",
    "from diff_gfdn.plot import plot_learned_svf_response\n",
    "plot_learned_svf_response(config_dict.num_groups, config_dict.sample_rate, \n",
    "                          output_biquad_coeffs, pos_to_investigate, svf_params=svf_params, save_path=f'{fig_path}/{config_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot the amplitude distribution for each RIR as a position of space for 1kHz band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_gfdn.plot import plot_amps_in_space\n",
    "\n",
    "plot_amps_in_space(room_data, all_rirs, all_pos, scatter=True, save_path=f'{fig_path}/{config_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
