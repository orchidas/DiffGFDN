from typing import Dict, List, Tuple, Union

import numpy as np
import torch
from numpy.typing import ArrayLike
from torch import nn


def db(x: torch.tensor,
       is_squared: bool = False,
       min_value: float = -200) -> torch.tensor:
    """Convert values to decibels.

    Args:
        x (torch.tensor):
            value(s) to be converted to dB.
        is_squared (bool):
            Indicates whether `x` represents some power-like quantity (True) or some root-power-like quantity (False).
            Defaults to False, i.e. `x` is a root-power-like auqntity (e.g. Voltage, pressure, ...).
        min_value (float): cap the decibels to this value, cannot be lower

    Returns:
        An array with the converted values, in dB.
    """
    x = torch.abs(x)
    factor = 10.0 if is_squared else 20.0
    y = factor * torch.log10(x + torch.finfo(torch.float32).eps)
    return y.clip(min=min_value)


def db2lin(
        x: Union[torch.tensor, ArrayLike]) -> Union[torch.tensor, ArrayLike]:
    """Convert from decibels to linear

    Args:
        x (ArrayLike): value(s) to be converted

    Returns:
        (ArrayLike): values converted to linear
    """
    return torch.power(10.0, x * 0.05)


def ms_to_samps(ms: Union[float, ArrayLike],
                fs: float) -> Union[int, ArrayLike]:
    """
    Convert ms to samples
    Args:
        ms (float or ArrayLike): time in ms
        fs (float): sampling rate
    Returns:
        int, ArrayLike: time in samples
    """
    samp = ms * 1e-3 * fs
    if np.isscalar(samp):
        return samp.astype(np.int32)
    else:
        return int(samp)


@torch.no_grad()
def get_response(x: Dict, net: nn.Module):
    """
    Get impulse and magnitude resoponse generated by the learned parameters of the DiffGFDN
    Args    net (nn.Module): trained GFDN network
            x (Dict): dictionary of features and labels
    Output  h (torch.tensor): GFDN impulse response
            H (torch.tensor): GFDN frequency response
    """
    with torch.no_grad():
        H = net(x)
        H = torch.sum(H, dim=-1)
        h = torch.fft.irfft(H)
    return H, h


def get_str_results(epoch=None,
                    train_loss=None,
                    time=None,
                    lossF=None,
                    lossT=None):
    """Construct the string that has to be print at the end of the epoch"""
    to_print = ''

    if epoch is not None:
        to_print += 'epoch: {:3d} '.format(epoch)

    if train_loss is not None:
        to_print += '- train_loss: {:6.4f} '.format(train_loss[-1])

    if time is not None:
        to_print += '- time: {:6.4f} s'.format(time)

    if lossF is not None:
        to_print += '- lossF: {:6.4f}'.format(lossF)

    if lossT is not None:
        to_print += '- lossT: {:6.4f}'.format(lossT)

    return to_print


def hermitian_conjugate_polynomial_matrix(A: torch.tensor) -> torch.tensor:
    """
    For a polynomail matrix A(z), calculate A(z^{-1})^H
    Size of the matix is N x N x p, with polynomials along the last axis
    """
    Aconj = torch.conj(torch.flip(A, -1))
    Aconj = Aconj.permute(1, 0, 2)
    return Aconj


def matrix_convolution(A: torch.tensor, B: torch.tensor) -> torch.tensor:
    """
    Take 2 polynomial matrices of orders (M, N, K) and (P, Q, R)
    and convolve them to produce a polynomial matrix of order (M, Q, R+K-1)
    """
    M, N, K = A.shape
    P, Q, R = B.shape
    assert N == P, "matrices must be commutable"

    C = torch.zeros((M, Q, K + R - 1))
    A = A.permute(2, 0, 1)
    B = B.permute(2, 0, 1)
    C = C.permute(2, 0, 1)

    for row in range(M):
        for col in range(Q):
            for it in range(N):
                C[:, row, col] += torch.conv1d(A[:, row, it], B[:, it, col])

    C = C.permute(1, 2, 0)
    return C


def to_complex(X: torch.Tensor):
    """Make a real tensor complex"""
    return torch.complex(X, torch.zeros_like(X))


def is_paraunitary(A: torch.tensor, max_tol: float = 1e-9) -> bool:
    """
    Check if a polynomial matrix A of size NxNxP is paraunitary by ensuring
    A(z) A(z^{-1})^H = I
    Args:
        A(NDArray): polynomial matrix of order NxNxp
        max_tol (float): maximum deviation from identity matrix
    Returns:
        bool: whether the matrix is PU
    """
    N = A.shape[0]
    p = A.shape[-1]

    # A(z^{-1})^H
    Aconj = hermitian_conjugate_polynomial_matrix(A)
    # A(z) A(z^{-1})^H
    T = matrix_convolution(A, Aconj)

    # must be close to 0
    T[:, :, p - 1] = T[:, :, p - 1] - torch.eye(N)
    max_off_diag_value = torch.max(np.torch(T))
    return max_off_diag_value < max_tol


def is_unitary(A: torch.tensor, max_tol: float = 1e-9) -> bool:
    """Check if a square matrix A is unitary"""
    N = A.shape[0]
    Aconj = torch.conj(A.T)
    T = torch.mm(A, Aconj)
    T -= torch.eye(N)
    max_off_diag_value = torch.max(torch.abs(T))
    return max_off_diag_value < max_tol


def absorption_to_gain_per_sample(room_dims: Tuple, absorption_coeff: float,
                                  delay_length_samp: List[int],
                                  fs: float) -> Tuple[float, List]:
    """
    Use Sabine's equation to get T60 from absorption coefficient, then convert that to the equivalent
    gain for a delay line, given its length in samples.
    Args:
        room_dims (Tuple): room dimensions for a room as a tuple of length, width, height
        absorption_coeff (float): uniform absorption coefficient for a room
        delay_length_samp (int): length of the delay lines in samples 
        fs (float): sampling rate
    Returns:
        Tuple: RT60s and list of gain per sample (1 for each room)
    """
    volume = np.prod(room_dims)
    if len(room_dims) == 3:
        area = 2 * (room_dims[0] * room_dims[1] + room_dims[1] * room_dims[2] +
                    room_dims[2] * room_dims[0])
    else:
        area = 2 * (room_dims[0] + room_dims[1])

    # RT60 according to sabine
    rt60 = 0.161 * volume / (area * absorption_coeff)
    gain_per_sample = db2lin(-60 * delay_length_samp / (fs * rt60))

    return (rt60, gain_per_sample)
